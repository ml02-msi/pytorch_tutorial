{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!which python3",
   "id": "63c1774c1cc446f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!python3 --version",
   "id": "3c91d5f6a13fe36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import os",
   "id": "ba84a40f09eca9e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import torch",
   "id": "934b5759cb7712a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(torch.cuda.is_available())",
   "id": "6f78b7638e87b7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ],
   "id": "ee12324fc89137d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(torch.__version__)",
   "id": "37ced9ffe029d5d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(torch.cuda.get_device_name(0))",
   "id": "349ee048677af568",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TO create an empty tensor\n",
    "\n",
    "torch.empty(2,3)"
   ],
   "id": "6317a5812f7fd447",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "type(torch.empty(2,3))",
   "id": "d4ff631a217509eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.empty(2,3)",
   "id": "81561249ee61bfbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.zeros(2,3)",
   "id": "54a1617d433895b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.ones(2,3)",
   "id": "c1cac3fccead2c66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.rand(2,3)",
   "id": "78fbd6d42a42d823",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "torch.rand(2,3)"
   ],
   "id": "edfa1d1a0669cff7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.rand(2,3)",
   "id": "9812194369ceace8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.tensor([1,2,3,4])",
   "id": "dddea0e1f0251706",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.arange(0,10,2)",
   "id": "9e72c65f19dcd977",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.linspace(0,10,10) # Same linearly separated points b/w a range inclusive",
   "id": "9efeb29b0068dd48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.eye(3) #identity tensor",
   "id": "6c139789124277a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.full((3,3),1) # Populates a given shape of tensor with a particular value",
   "id": "465f32cea4fc199c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# tensor shapes\n",
    "\n",
    "x = torch.tensor([[[1,2,3,]]])\n",
    "print(x.shape)"
   ],
   "id": "dc45538eadf27d32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# To create a empty tensor with a particular shape\n",
    "\n",
    "torch.empty_like(x)"
   ],
   "id": "d120a926d017e6f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.zeros_like(x)",
   "id": "89674ecbecddfe20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.ones_like(x)",
   "id": "952f4f3fc18adff7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.rand_like(x) # TO generate this we have to implicitly declare the data type (here float as rand generates float)",
   "id": "52141443b9e47a8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x.dtype",
   "id": "3de52d5b6b389672",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.tensor([1,2,3,4],dtype=torch.uint8)",
   "id": "56106cf48ee20671",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "x_quant = torch.quantize_per_tensor(x, scale=1.0, zero_point=0, dtype=torch.quint4x2)\n",
    "x_quant"
   ],
   "id": "62e952711d230270",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.tensor([1,2,3,4],dtype=torch.cuda.IntTensor)",
   "id": "8950120126883b7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor([1, 2, 3, 4])\n",
    "print(x.device)  # Output: cpu\n"
   ],
   "id": "25863db0abeabf78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor([1, 2, 3, 4], device='cuda')  # Goes to GPU\n",
    "print(x.device)  # Output: cuda:0\n"
   ],
   "id": "91f429a5a1262656",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor([1, 2, 3, 4])\n",
    "x = x.to('cuda')\n",
    "x.device"
   ],
   "id": "639101e321c63a0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x = torch.cuda.FloatTensor(3, 2)  # 3x2 float tensor on GPU with uninitialized values\n",
   "id": "fd0ab0d2c4db99b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[1,2,3],[2,3,4]], dtype=torch.float64)\n",
    "x"
   ],
   "id": "de427d848a5da637",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x = x.to(torch.int)",
   "id": "a14fa9558f1e2d92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.rand_like(x, dtype=torch.float32)",
   "id": "4bd9f79e22691f61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Scalar operations\n",
    "print(x.dtype)\n",
    "\n",
    "print(x)"
   ],
   "id": "b0a071bb46e71991",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(x+2)\n",
    "\n",
    "print(x-2)"
   ],
   "id": "a2efc89378c0ce7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(x*2)\n",
    "\n",
    "print(x/2)"
   ],
   "id": "62d0a6bb11f2fa0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print((x*100)//2)\n",
    "\n",
    "print(((x*100)//3)%2)"
   ],
   "id": "44b479f8d3889274",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(x**2)",
   "id": "5203556916aa2f4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Element Wise Ops\n",
    "\n",
    "x = torch.rand_like(x, dtype=torch.float32)\n",
    "y = torch.randn_like(x, dtype=torch.float32)\n",
    "\n",
    "print(x,y)"
   ],
   "id": "f52adddc63bd7f01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x+y",
   "id": "553971717a12e66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x-y",
   "id": "3be73e816bcd0687",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x*y",
   "id": "e7df569d3d8c703a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x/y",
   "id": "a390a40f68433e71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "(x*100)//y",
   "id": "bfccaa6f0ea24158",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x%y",
   "id": "5659db69632b84f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.abs(x%y)",
   "id": "793c4a8f5f193d51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.round(torch.abs(x%y))",
   "id": "7e42c29f3cbcd0cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.ceil(x%y)",
   "id": "66c0df03321fbac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.floor(x%y)",
   "id": "d0b2c18b44757131",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.clamp(x%y, min=-0.02, max=0.5)",
   "id": "e993c4a952d66bcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reduction op\n",
    "e = torch.randint(size = (2,3), low=5, high=10, device='cuda')\n",
    "print(e)"
   ],
   "id": "fb1bdb57f261ca25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.sum(e)",
   "id": "39906a6a389dd5ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(torch.sum(e, dim=0)) # Sum along each column\n",
    "print(torch.sum(e, dim=1)) # Sum along each row"
   ],
   "id": "59b91e51d5a74a44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(torch.mean(e, dtype = torch.float)) # Works with dtype = float\n",
    "print(torch.mean(e, dtype = torch.float, dim = 0))"
   ],
   "id": "767e19acdc7d4cc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(torch.median(e)) # Works with dtype = float",
   "id": "448cb86f150755bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(torch.median(e, dim = 0))",
   "id": "58e61166830a09eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(torch.min(e)) # Works with dtype = float\n",
    "print(torch.max(e, dim = 0))"
   ],
   "id": "c1444f0d856c39c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# unpacking\n",
    "\n",
    "values, indices = torch.min(e, dim=0)\n",
    "\n",
    "print(f'The values are: {values}')\n",
    "print(f'The indices are: {indices}')"
   ],
   "id": "7e8fb9e2d1627397",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Variance\n",
    "## std and var only support floating point and complex dtypes\n",
    "torch.var(e.to(torch.float))"
   ],
   "id": "7484cae30418ce43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.var(e.float())",
   "id": "f04abc2ff1111df8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.var(e.float().cuda())",
   "id": "80a3660cce9df1e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.var(e.float().cpu())",
   "id": "254cc20d2b33b851",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "e",
   "id": "a29936dc33df7838",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.argmax(e) # Returns the pos of the maximum elem",
   "id": "13cce4f2183cea5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.argmin(e)",
   "id": "d02497abbaede505",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Matrix Multiplication\n",
    "print(x.shape)\n",
    "torch.matmul(x,y.view(3,2)) # Reshaping the y tensor so that mat mul can take place"
   ],
   "id": "5869bab807cc3d47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.dot(x.view(-1),y.view(-1))  # Squeezing both to 1D tensor",
   "id": "7b1f942273864c36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y",
   "id": "3456ce5a18cb5c8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.transpose(y, 0, 1)",
   "id": "bacd9f5f9175d966",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.transpose(y, 1, 0)",
   "id": "bb42d9e0b9d68a29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.unsqueeze(y, 1)",
   "id": "4f343addb74fa02d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x",
   "id": "e5e7fb98c74577e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create 1x3 row of random numbers (same dtype and device as x)\n",
    "rand_row = torch.randn(1, 3, dtype=x.dtype, device=x.device)\n",
    "\n",
    "print(rand_row)\n",
    "\n",
    "# Concatenate along the row axis (dim=0)\n",
    "x_3x3 = torch.cat((x, rand_row), dim=0)\n",
    "\n",
    "print(x_3x3)\n"
   ],
   "id": "bdcfb49299986d01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.det(x_3x3)",
   "id": "611ad95b7c419d47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.inverse(x_3x3)",
   "id": "9c1c6da710a0e8a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(x)\n",
    "print(y)"
   ],
   "id": "7ec98b3351d6f2c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Comparison Operations\n",
    "\n",
    "x > y"
   ],
   "id": "a84a19f2e0d94acb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x<y",
   "id": "cbe4b81f867e7bfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x==y",
   "id": "a527facdc2489198",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x!=y",
   "id": "f1486b86fc31b6c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x>=y",
   "id": "2765370a63d7515b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x<=y",
   "id": "c4a7cf5454d1a7a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Special Funcs\n",
    "k = torch.randint(size=(3,3), low = -1, high =11)\n",
    "k"
   ],
   "id": "69524a09e49b86eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.log(k)",
   "id": "15b85dfa3ac95f23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.exp(k)",
   "id": "1b355eeb791ba197",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.sqrt(k)",
   "id": "9e8e1930762a8163",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.sigmoid(k)",
   "id": "db8bf6f59d157c4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.softmax(k.to(float), dim = 0) # Expects tensor dtype as float",
   "id": "fef0dc2d87098ff7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.relu(k)",
   "id": "58943bc2964776c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inplace Operators ('_' underscore defines inplace operators)\n",
    "print(x)\n",
    "print(y)\n",
    "print(x.add(y))"
   ],
   "id": "9e27ca2078de7a6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(x)\n",
    "print(x.add_(y))\n",
    "print(x)"
   ],
   "id": "be0b314930166a39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(k)",
   "id": "2f602dedbc5d162b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(torch.relu(k))",
   "id": "f93b12678cef1976",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(torch.relu_(k)) # Or do k.relu_()\n",
    "print(k)"
   ],
   "id": "d9c00e8823515e5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# copying a tensor\n",
    "\n",
    "x = torch.rand(2,3)\n",
    "print(x)\n",
    "y = x\n",
    "print(y)"
   ],
   "id": "b06f2640b293127d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x[0][0] = 0\n",
    "print(x)\n",
    "print(y)"
   ],
   "id": "1b87575cdd7e1e8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "id(x) == id(y) # The memory they are pointing to is the same, so if we change one the other will get changed",
   "id": "b9b9a976a7505b6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y[0][0] = 1\n",
    "print(x)\n",
    "print(y)"
   ],
   "id": "54f28d835e22160",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Copying a tensor but both will be pointing to different locations\n",
    "\n",
    "a = torch.rand(2,3)\n",
    "b = a.clone()\n",
    "print(id(a))\n",
    "print(id(b))"
   ],
   "id": "2679ba18064fd0f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import copy\n",
    "a = torch.rand(2,3)\n",
    "b = copy.deepcopy(a)\n",
    "print(id(a))\n",
    "print(id(b))\n",
    "print(type(b))"
   ],
   "id": "96d33d02a454cae4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "size = 10000\n",
    "\n",
    "matrix_cpu1 = torch.randn(size, size)\n",
    "matrix_cpu2 = torch.randn(size, size)\n",
    "\n",
    "start_time = time.time()\n",
    "result_cpu = torch.matmul(matrix_cpu1, matrix_cpu2)\n",
    "end_time = time.time()\n",
    "print(f'CPU time to multiply two 10000*10000 matrix is: {end_time - start_time}')\n",
    "\n",
    "matrix_gpu1 = torch.randn(size, size, device='cuda')\n",
    "matrix_gpu2 = torch.randn(size, size, device='cuda')\n",
    "\n",
    "start_time = time.time()\n",
    "result_gpu = torch.matmul(matrix_gpu1, matrix_gpu2)\n",
    "end_time = time.time()\n",
    "print(f'GPU time to multiply two 10000*10000 matrix is: {end_time - start_time}')"
   ],
   "id": "a9eb888954bd9d77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reshaping Tensors\n",
    "\n",
    "a = torch.ones(4,4)\n",
    "print(a)\n",
    "\n",
    "a = a.reshape(2,2,2,2)\n",
    "print(a)"
   ],
   "id": "bf64c4ef63c9fe0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a.flatten()",
   "id": "d7e19fd1b3ffb5b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "b = torch.randn(3,4,2)\n",
    "print(b)"
   ],
   "id": "598e14a4be99d267",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "b.permute(2,0,1) # takes the indices of the shape",
   "id": "b5378d2c309cc596",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "b.permute(2,0,1).shape",
   "id": "689534d1aac21089",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "a = torch.randn(226,226,3)\n",
    "a.unsqueeze(3).shape #Adds dimension to the existing shape at a particular position\n",
    "\n",
    "# unsqueeze is helpful in a scenario where if one single image is passed to a model, as a model takes images in batches\n",
    "# e.g. n*x*y*c where n is the batch number then in that case we can convert the single image of dim x*y*c into\n",
    "# a single batch like 1*x*y*c using unsqueeze"
   ],
   "id": "66b1201763f956d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Squeeze only removes the dimension if its size is 1.\n",
    "#\n",
    "# If tensor.shape[dim] != 1, it leaves the tensor unchanged.\n",
    "\n",
    "print(a.shape)\n",
    "print(a.squeeze(1).shape)"
   ],
   "id": "44c710f695d50713",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "a = torch.rand(1,20)\n",
    "print(a.squeeze(0).shape)"
   ],
   "id": "4a1d63890813aa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#  To remove all dimensions of size 1:\n",
    "# a.squeeze().shape  # removes all 1-sized dims, if any\n",
    "\n",
    "a = torch.rand(1,20,1)\n",
    "print(a.squeeze().shape)"
   ],
   "id": "fb5d183733d44c3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "b4eb1d817b330688",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "a = torch.tensor([1,2,3,4])\n",
    "a.type()"
   ],
   "id": "c5eec69df7ba3a4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "b = a.numpy()\n",
    "type(b)"
   ],
   "id": "c0f87be8e175a1e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "c = torch.from_numpy(b)\n",
    "c.type()"
   ],
   "id": "c0d54a0314d7ef68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"Autograd - To calculate derivatives\n",
    "Autograd is a core component of PyTorch that provides automatic differentiation for tensor ops, enabling gradient\n",
    "computation using optimization algo like gradient descent.\"\"\""
   ],
   "id": "82b49474834b4a3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "y = x**2\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ],
   "id": "f4eb952409cb9540",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " y.backward() # Calculating Derivative\n",
    " x.grad # To see derivative"
   ],
   "id": "a66b1f6c87fc5053",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x**2\n",
    "z = torch.sin(y)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ],
   "id": "85966fcbdcbfa592",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "z.backward()",
   "id": "42a044a993337759",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x.grad",
   "id": "ce5c69eda577171",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = torch.pow(x, 2)\n",
    "z = torch.sin(y)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ],
   "id": "b0bb828309f75f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "z.backward()",
   "id": "b27dcb9094829e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x.grad",
   "id": "62d1c3bb81740ce2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y.grad",
   "id": "c8b933afc2f03263",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inp = torch.eye(6, 5, requires_grad=True)\n",
    "print(inp)"
   ],
   "id": "652be6e8d4f25540",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test = torch.randn(5, 5)\n",
    "print(test)\n",
    "out = torch.matmul(inp, test)\n",
    "print(out)"
   ],
   "id": "f655ecf810dad2fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test[-1][-1]==out[0][0]",
   "id": "36572741c7dd6d70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Loss Function\n",
    "\n",
    "$L = -[y_{target}*\\ln(y_{pred}) + (1-y_{target})*\\ln(1-y_{pred})]$"
   ],
   "id": "abb157c449907a45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Inputs\n",
    "x = torch.tensor(6.7)\n",
    "y = torch.tensor(0.0)\n",
    "\n",
    "w = torch.tensor(1.0) # Weight\n",
    "b = torch.tensor(0.0) # bias"
   ],
   "id": "68e78a9dda07db19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Binary cross entropy loss for scalar\n",
    "def binary_cross_entropy_loss(prediction, target):\n",
    "    epsilon = 1e-8 # To prevent log(0)\n",
    "    prediction = torch.clamp(prediction, epsilon, 1-epsilon)\n",
    "    \"\"\"\" clamp(input, min=None, max=None, *, out=None) -> Tensor\n",
    "    Clamps all elements in :attr:`input` into the range min and max\"\"\"\n",
    "    return -(target*torch.log(prediction) + (1-target)*torch.log(1-prediction))"
   ],
   "id": "88b96d3d72c38282",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Forward pass\n",
    "z = w*x +b # Linear part of weighted sum\n",
    "y_pred = torch.sigmoid(z)\n",
    "\n",
    "#compute bce loss\n",
    "loss = binary_cross_entropy_loss(y_pred, y)"
   ],
   "id": "be0e850f7401c6c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "loss",
   "id": "92ee46724cef2813",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Backpropagation\n",
    "# Derivatives:\n",
    "\n",
    "# 1. dL/d(y_pred): Loss with respect to the prediction (y_pred)\n",
    "dloss_dy_pred = (y_pred - y)/(y_pred*(1-y_pred))\n",
    "\n",
    "# 2. dy_pred/dz: Prediction (y_pred) with respect to z (sigmoid activation)\n",
    "\n",
    "dy_pred_dz = y_pred * (1 - y_pred)\n",
    "\n",
    "# 3. dz/dw and dz/db: z with respect to w and b\n",
    "dz_dw = x\n",
    "dz_db = 1 # bias contributes directly to z\n",
    "\n",
    "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
    "dL_db = dloss_dy_pred * dy_pred_dz * dz_db"
   ],
   "id": "dfb7d7e3f22a5295",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Manual Gradient of loss wrt weight (dw): {dL_dw}\")\n",
    "print(f\"Manual Gradient of loss wrt bias (db): {dL_db}\")"
   ],
   "id": "794b04252768b5a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now using autograd doing this same neural network\n",
    "\n",
    "\"\"\"\n",
    "We do not need to calculate derivative wrt x or y,\n",
    "so we do not need to use requires_grad while defining x or y\n",
    "\"\"\"\n",
    "x = torch.tensor(6.7)\n",
    "y = torch.tensor(0.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True) # Weight\n",
    "b = torch.tensor(0.0, requires_grad=True) # bias"
   ],
   "id": "8c7af85ee03d9c8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Forward Propagation\n",
    "\n",
    "z = w*x +b\n",
    "z"
   ],
   "id": "91810a3a1e4f28d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_pred = torch.sigmoid(z)\n",
    "y_pred"
   ],
   "id": "b3ff73271f764fb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss = binary_cross_entropy_loss(y_pred,y)\n",
    "loss"
   ],
   "id": "f764e433617cb464",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Backpropagation\n",
    "\n",
    "loss.backward()"
   ],
   "id": "bb4ae04a8c661919",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Manual Gradient of loss wrt weight (dw): {w.grad}\")\n",
    "print(f\"Manual Gradient of loss wrt bias (db): {b.grad}\")"
   ],
   "id": "54bc42d72e917093",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We can also calculate gradient for vectors also\n",
    "## NB - RuntimeError: Only Tensors of floating point and complex dtype can require gradients\n",
    "\n",
    "x = torch.tensor([1.,2.,3.], requires_grad=True)\n",
    "y = (x**2).mean()\n",
    "\n",
    "y.backward() # Computes partial differentiation wrt vector\n",
    "x.grad"
   ],
   "id": "31e9cc64367a5e54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Gradients accumulates, we have to clear grads\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "x"
   ],
   "id": "188184ba45c3a42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Forward pass\n",
    "y = (x**2)\n",
    "y"
   ],
   "id": "28ca6b9605594de6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y.backward()",
   "id": "7a62623b49e92197",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x.grad # The value of x.grad gets add during every pass",
   "id": "d330b01a5211f823",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x.grad.zero_() # After every epoch we clear gradients using this",
   "id": "368ab8857b24b740",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Also sometime we do not need to calculate derivatives eg in the time of predictions we do not need gradients\n",
    "# requires_grad takes unnecessary memory space in these situations, we clear it by disabling it\n",
    "# disable gradient tracking\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "x"
   ],
   "id": "71536904319b7b8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Forward pass\n",
    "y = (x**2)\n",
    "y"
   ],
   "id": "efd9c60b204509e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y.backward()\n",
    "x.grad"
   ],
   "id": "cbdbd955f8130586",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Option 1: requires_grad_(False)",
   "id": "f2f68f85d82a5d0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "x"
   ],
   "id": "bacfae24ade08b34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x.requires_grad_(False)",
   "id": "7a02ad6daa9c69fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y = x**2",
   "id": "8f596b666162728c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# y.backward()\n",
    "# Output: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn"
   ],
   "id": "e75fb4be1895124b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Option 2: detach()",
   "id": "2b34e0e9d1e8d95d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "x"
   ],
   "id": "5808bc08b18fbb6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z = x.detach()\n",
    "z"
   ],
   "id": "e30d294914ceb76f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y = x**2\n",
    "y1 = z**2\n",
    "print(y)\n",
    "print(y1)"
   ],
   "id": "642651ff25a237a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Option 3: torch.no_grad()",
   "id": "9e3ba6c54d134c00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "x"
   ],
   "id": "b914c01bdb81909f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    y = x**2\n",
    "y"
   ],
   "id": "387dfc465369f21e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# training pipeline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ],
   "id": "7f78c3856723c12f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
    "df.head()"
   ],
   "id": "fb2017816e6fa300",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.columns",
   "id": "ccef01515e7594a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "1ae048b6d4800339",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove unwanted columns\n",
    "\n",
    "df = df.drop(columns=['id','Unnamed: 32'])"
   ],
   "id": "e809ea51d163ddd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.columns",
   "id": "25b9d42aa30fc869",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "1295ae8df85f41a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.iloc[:,1:] # All rows, from 2nd columns",
   "id": "8ab0f6da4f421ea3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.iloc[:,0] # All rows, only first column",
   "id": "f513cc1f07b945a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:], df.iloc[:,0], test_size=0.2)",
   "id": "260dba5a714497ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Scaling\n",
    "\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.transform(X_test)"
   ],
   "id": "f744f3873fac777e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Label Encoding\n",
    "\n",
    "# \"The model might mis-interpret this encoding to be ordinal nature that is 1>0,\n",
    "# will try to use one-hot encoding here afterward\" - This ides is wrong here;\n",
    "# label encoding is appropriate here as we have only two classes i.e. binary - it's just binary to the model\n",
    "# Models will learn to predict either 0 or 1.; No false ordering issue since there are only two options.\n",
    "# NB - Most PyTorch loss functions (like nn.BCEWithLogitsLoss or nn.CrossEntropyLoss) expect class labels as integers.\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ],
   "id": "a23d1f11f75b23f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_train",
   "id": "a4e59ea415471537",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Converting all these numpy arrays to torch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)"
   ],
   "id": "6737b611367b1f35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(X_train_tensor.shape)\n",
    "print(y_train_tensor.shape)"
   ],
   "id": "e3b51604761e8321",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train_tensor[0][0]",
   "id": "30a51de9a1e67e00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Defining the model using python OOP to implement training in pytorch\n",
    "# We will be using a single neuron here and it will take input directly from our training data\n",
    "# We have 30 features in our training dataset, so our simple NN here will have 30 weights\n",
    "# and one bias\n",
    "\n",
    "class MySimpleNN():\n",
    "    # defining a constructor\n",
    "    def __init__(self, X):\n",
    "        # Creating a weight matrix of size 30x1 (30 input feature); float64 because the training data has same dtype\n",
    "        self.weights = torch.rand(X.shape[1], 1, dtype = torch.float64, requires_grad=True)\n",
    "        # As we have only 1 neuron we require only 1 item\n",
    "        self.bias = torch.zeros(1, dtype = torch.float64, requires_grad=True)\n",
    "    def forward(self, X):\n",
    "        z = torch.matmul(X, self.weights) + self.bias\n",
    "        y_pred = torch.sigmoid(z)\n",
    "        return y_pred\n",
    "    # As we are working on a classification problem we will use bce loss\n",
    "    def loss(self, y_pred, y):\n",
    "        # Clamp predictions to avoid log(0)\n",
    "        epsilon  = 1e-7\n",
    "        y_pred = torch.clamp(y_pred, epsilon, 1-epsilon)\n",
    "        # calculating the mean to find the overall loss\n",
    "        loss = -(y*torch.log(y_pred) + (1-y)*torch.log(1-y_pred)).mean()\n",
    "        return loss\n",
    "\n"
   ],
   "id": "617354420265d793",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# defining params\n",
    "\n",
    "lr = 0.1\n",
    "epochs = 25"
   ],
   "id": "d888e4eb6c5990b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The formula to calculate new weights is -\n",
    "\n",
    "$ W_{\\text{new}} = W_{\\text{old}} - lr \\cdot \\frac{\\partial L}{\\partial w} $\n",
    "\n",
    "The formula to calculate new bias is -\n",
    "\n",
    "$ b_{\\text{new}} = b_{\\text{old}} - lr \\cdot \\frac{\\partial L}{\\partial b} $\n",
    "\n"
   ],
   "id": "b70238700013475c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training pipeline\n",
    "\n",
    "# create model\n",
    "# to create a model we have to follow mainly four steps and these four steps will be in a loop of epoch times\n",
    "# 1. Forward Pass > 2. Calculate Loss > 3. Backpropagation(Differentiation takes place here)\n",
    "# > 4. Update params like weights and biases\n",
    "\n",
    "model = MySimpleNN(X_train_tensor)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward Pass (calculate wx+b and calculate sigmoid)\n",
    "    y_pred = model.forward(X_train_tensor)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = model.loss(y_pred, y_train_tensor)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # update params\n",
    "    # In parameters requires grad is set to true; but in this operation we don't need to calculate the gradient\n",
    "    with torch.no_grad():\n",
    "        model.weights -= lr*model.weights.grad\n",
    "        model.bias -= lr*model.bias.grad\n",
    "\n",
    "    # And also there is the problem of gradient accumulation so we set the grad to zero in every epoch\n",
    "    model.weights.grad.zero_()\n",
    "    model.bias.grad.zero_()\n",
    "\n",
    "    # print loss in each epoch\n",
    "    # if epoch % 100 == 0:\n",
    "    print(f'Epoch: {epoch+1}, loss: {loss.item()}')\n",
    "\n"
   ],
   "id": "2c39473bb08d9d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.bias",
   "id": "3f438651a105d04a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(model.weights.shape)\n",
    "print(model.bias.shape)"
   ],
   "id": "aeb3a1a8f04f27f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluation\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model.forward(X_test_tensor)\n",
    "    # y_pred will have value b/w 0 and 1 but we will apply threshold to be able to compare it with original labels\n",
    "    y_pred = (y_pred > 0.9).float()\n",
    "    accuracy = (y_pred == y_test_tensor).float().mean()\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n"
   ],
   "id": "48416148a1aeb804",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now we will try to do some improvements\n",
    "# 1. Building the NN using torch.nn module\n",
    "# 2. Using built-inn activation fnc\n",
    "# 3. Using Built-in loss function\n",
    "# 4. Using built-in optimizer\n",
    "\n",
    "# Modules(layers) - nn.Linear(fully connected layers), nn.Conv2d, nn.LSTM(recurrent layer) etc\n",
    "# Activation Fnc - nn.ReLU, nn.Sigmoid, nn.Tanh\n",
    "# Loss Fnc - nn.CrossEntropy, nn.MSELoss, nn.NLLLoss\n",
    "# Container Modules - nn.Sequential - to stack layers in order\n",
    "# Regularization & Droupout - nn.Dropout, nn.BatchNorm2d help prevent overfitting"
   ],
   "id": "80c5b3b6984a7dc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "id": "11fe56e983f20cdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating the base class, which will have all the functionality of the neural network\n",
    "# The class will inherit from nn.Module to get its offerings\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__() # Invoking the constructor of the parent class with the help of super keyword\n",
    "        # i.e. it is inheriting the properties of nn.module by this\n",
    "        # Now creating a single neuron\n",
    "        self.linear = nn.Linear(num_features, 1, bias=True)  # learns an additive bias (default)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, features):\n",
    "        out = self.linear(features) # calculating z = wx+b here\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ],
   "id": "308b58340c5dc0f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating fake dataset\n",
    "\n",
    "features = torch.rand(10,5)\n",
    "\n",
    "model = Model(features.shape[1]) # taking 5 as features\n",
    "\n",
    "# model.forward(features) # this also works\n",
    "\n",
    "# pytorch uses a magic function named __call__ to override nn.module class\n",
    "# and to invoke forward method whenever any class object is called ( recommended by pytorch)\n",
    "print(model(features))"
   ],
   "id": "6eab80becc498507",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TO show model weight and bias\n",
    "\n",
    "print(model.linear.weight)\n",
    "print(model.linear.bias)"
   ],
   "id": "9ed05083ed8650ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(10,5))"
   ],
   "id": "2dff834b322eed90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now lets introduce a hidden layer with 3 neurons\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        # hidden layer  is taking 5 input features and giving output 3 weights\n",
    "        self.linear1 = nn.Linear(num_features, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(3,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, features):\n",
    "        out = self.linear1(features)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ],
   "id": "21b8e5b2669815fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating fake dataset\n",
    "\n",
    "features = torch.rand(10,5)\n",
    "\n",
    "model = Model(features.shape[1]) # taking 5 as features\n",
    "\n",
    "print(model(features))"
   ],
   "id": "6c1cd98324a75884",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TO show model weight and bias\n",
    "\n",
    "print(model.linear1.weight)\n",
    "print(model.linear1.bias)"
   ],
   "id": "27e378e4f3d5850f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TO show model weight and bias\n",
    "\n",
    "print(model.linear2.weight)\n",
    "print(model.linear2.bias)"
   ],
   "id": "b00697bc471c4d9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(10,5))"
   ],
   "id": "2b31ce9029327999",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now we will use Sequential container\n",
    "\n",
    "# Now lets introduce a hidden layer with 3 neurons\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        # hidden layer  is taking 5 input features and giving output 3 weights\n",
    "        # Sequential Container\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(num_features, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        out = self.network(features)\n",
    "        return out"
   ],
   "id": "f224f6531f69ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating fake dataset\n",
    "\n",
    "features = torch.rand(10,5)\n",
    "\n",
    "model = Model(features.shape[1]) # taking 5 as features\n",
    "\n",
    "print(model(features))"
   ],
   "id": "7a4f22e3d02be79a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# new training pipeline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ],
   "id": "d8b902640fff498c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
    "df.head()"
   ],
   "id": "c51709ff74fff337",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove unwanted columns\n",
    "\n",
    "df = df.drop(columns=['id','Unnamed: 32'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:], df.iloc[:,0], test_size=0.2)\n",
    "\n",
    "# Scaling\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.transform(X_test)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test_tensor = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train_tensor = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test_tensor = torch.from_numpy(y_test.astype(np.float32))"
   ],
   "id": "860339310e4ea750",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Defining our refined model\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_features, 1) # Uses single neuron\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, features):\n",
    "        out = self.linear(features)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    def loss(self, y_pred, y):\n",
    "        # Clamp predictions to avoid log(0)\n",
    "        epsilon  = 1e-7\n",
    "        y_pred = torch.clamp(y_pred, epsilon, 1-epsilon)\n",
    "        # calculating the mean to find the overall loss\n",
    "        loss = -(y*torch.log(y_pred) + (1-y)*torch.log(1-y_pred)).mean()\n",
    "        return loss\n"
   ],
   "id": "c09f60f06f766057",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# defining params\n",
    "\n",
    "lr = 0.1\n",
    "epochs = 25"
   ],
   "id": "a2e7ef5bf81412e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# creating model\n",
    "\n",
    "model = Model(X_train_tensor.shape[1])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward Pass (calculate wx+b and calculate sigmoid)\n",
    "    y_pred = model(X_train_tensor)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = model.loss(y_pred, y_train_tensor)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # update params\n",
    "    # In parameters requires grad is set to true; but in this operation we don't need to calculate the gradient\n",
    "    with torch.no_grad():\n",
    "        model.linear.weight -= lr*model.linear.weight.grad\n",
    "        model.linear.bias -= lr*model.linear.bias.grad\n",
    "\n",
    "    # And also there is the problem of gradient accumulation so we set the grad to zero in every epoch\n",
    "    model.linear.weight.grad.zero_()\n",
    "    model.linear.bias.grad.zero_()\n",
    "\n",
    "    # print loss in each epoch\n",
    "    # if epoch % 100 == 0:\n",
    "    print(f'Epoch: {epoch+1}, loss: {loss.item()}')"
   ],
   "id": "dc522b70bf3a4246",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    # y_pred will have value b/w 0 and 1 but we will apply threshold to be able to compare it with original labels\n",
    "    y_pred = (y_pred > 0.9).float()\n",
    "    accuracy = (y_pred == y_test_tensor).float().mean()\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n"
   ],
   "id": "b2fc43d0c6338d30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.linear.weight # As it had 30 inputs",
   "id": "a30eebaa059f2bc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.linear.bias",
   "id": "d097dd082b13c320",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now using built-in loss function\n",
    "# defining our refined model\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_features, 1) # Uses single neuron\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, features):\n",
    "        out = self.linear(features)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ],
   "id": "27597e364e747b6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# defining params\n",
    "\n",
    "lr = 0.1\n",
    "epochs = 25"
   ],
   "id": "c0742dbb61833c14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "loss_fnc = nn.BCELoss() # this stores a callable fnc in loss_fnc variable",
   "id": "476ca715a0cb64f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "type(loss_fnc)",
   "id": "c06c9d073d4eea0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(y_pred.shape)\n",
    "print(y_train_tensor.shape)"
   ],
   "id": "4fff1a5f60a98138",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(y_train_tensor)\n",
    "print(y_train_tensor.shape)"
   ],
   "id": "44bd0b1666ddaf98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(y_train_tensor.reshape(-1,1))\n",
    "print(y_train_tensor.reshape(-1,1).shape)"
   ],
   "id": "da424e06416a7988",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(y_train_tensor.view(-1,1))\n",
    "print(y_train_tensor.view(-1,1).shape)"
   ],
   "id": "2c67acfbf8c539db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# creating model\n",
    "\n",
    "model = Model(X_train_tensor.shape[1])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward Pass (calculate wx+b and calculate sigmoid)\n",
    "    y_pred = model(X_train_tensor)\n",
    "\n",
    "    # updated calculate loss\n",
    "    \"\"\"using loss_fnc(y_pred, y_train_tensor) will throw the below error -\n",
    "    ValueError: Using a target size (torch.Size([455])) that is different to the input\n",
    "    size (torch.Size([455, 1])) is deprecated. Please ensure they have the same size.\n",
    "    We have to reshape y_train_tensor to the shape of y_pred i.e. [455, 1]\n",
    "    \"\"\"\n",
    "    # Using view to reshape is better for contiguous data\n",
    "    loss = loss_fnc(y_pred, y_train_tensor.view(-1,1))\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # update params\n",
    "    # In parameters requires grad is set to true; but in this operation we don't need to calculate the gradient\n",
    "    with torch.no_grad():\n",
    "        model.linear.weight -= lr*model.linear.weight.grad\n",
    "        model.linear.bias -= lr*model.linear.bias.grad\n",
    "\n",
    "    # And also there is the problem of gradient accumulation so we set the grad to zero in every epoch\n",
    "    model.linear.weight.grad.zero_()\n",
    "    model.linear.bias.grad.zero_()\n",
    "\n",
    "    # print loss in each epoch\n",
    "    # if epoch % 100 == 0:\n",
    "    print(f'Epoch: {epoch+1}, loss: {loss.item()}')"
   ],
   "id": "723d2dcb80b2d7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(model.linear.weight) # As it had 30 inputs\n",
    "print(model.linear.bias) # As it had 30 inputs"
   ],
   "id": "3f6253697d2281aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred = (y_pred > 0.5).float()\n",
    "    accu = (y_pred == y_test_tensor).float().mean()\n",
    "    print(f'Accuracy is: {accu:.4f}')\n"
   ],
   "id": "d7c1fb7c6d58cc46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now we will be using torch.optim\n",
    "\n",
    "# It helps to update params of model during training\n",
    "# It handles weight updates efficiently, including additional features like learning rate scheduling and weight decay(regularization)\n",
    "# Adam, SGD, RMSProp, Momentum-based optimization etc.\n",
    "\n",
    "# models.parameters() retrieves an iterator over all the trainable params(weights and biases) in a model\n",
    "# These params are instances of torch.nn.Parameter and include weights and biases to compute gradient and update during training\n",
    "# https://www.comet.com/site/blog/7-optimization-methods-used-in-deep-learning/#:~:text=Momentum%2Dbased%20optimization%20takes%20steps,value%20of%20the%20objective%20function\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, features):\n",
    "        out = self.linear(features)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n"
   ],
   "id": "76089d2ac386e909",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lr = 0.1\n",
    "epochs = 25\n",
    "\n",
    "loss_fnc = nn.BCELoss()\n",
    "\n",
    "# creating model\n",
    "model = Model(X_train_tensor.shape[1])\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward Pass (calculate wx+b and calculate sigmoid)\n",
    "    y_pred = model(X_train_tensor)\n",
    "\n",
    "    # updated calculate loss\n",
    "    \"\"\"using loss_fnc(y_pred, y_train_tensor) will throw the below error -\n",
    "    ValueError: Using a target size (torch.Size([455])) that is different to the input\n",
    "    size (torch.Size([455, 1])) is deprecated. Please ensure they have the same size.\n",
    "    We have to reshape y_train_tensor to the shape of y_pred i.e. [455, 1]\n",
    "    \"\"\"\n",
    "    # Using view for reshape is better for contiguous data\n",
    "    loss = loss_fnc(y_pred, y_train_tensor.view(-1,1))\n",
    "\n",
    "    # One Line of code to make the grad zero in every epoch through optimizer\n",
    "    # Also, good strategy is to clear gradient before backward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    # print loss in each epoch\n",
    "    # if epoch % 100 == 0:\n",
    "    print(f'Epoch: {epoch+1}, loss: {loss.item()}')"
   ],
   "id": "574d0b39f2945cd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred = (y_pred > 0.9).float()\n",
    "    accu = (y_pred == y_test_tensor).float().mean()\n",
    "    print(f'Accuracy is: {accu:.4f}')\n"
   ],
   "id": "182be00dac895ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# so, our training pipeline build so far has a flaw i.e. we are using batch gradient descent when training the model.\n",
    "# i.e. we are using our whole dataset at a time when updating our parameters\n",
    "# we are using whole dataset in a single forward pass, calculating loss on the whole dataset and we are calculating gradients and\n",
    "# updating params based on it\n",
    "# batch gradient descent is considered inefficient because it is memory inefficient and\n",
    "# it doesn't have a good convergence\n",
    "# like the model is seeing the data but is not optimising it in step but at once making the optimization slow;\n",
    "# optimization steps should be frequent that's why we use sgd(update param after seeing every row-making it converge faster)\n",
    "\n",
    "# we will use mini batch gradient descent i.e. it will divide the batch into mini batches\n",
    "\n",
    "# there could be simple solution to this where we use the mini batch gradient descent by doing manual code(calculating grad every n step)\n",
    "# but this could cause problem such as - 1) No standard interface for data (creates problem for large datasets)\n",
    "# 2) No easy way to apply transformation\n",
    "# 3) Shuffling and sampling\n",
    "# 4) Batch management and parallelization\n",
    "\n",
    "# That's why to handle this problem pytorch gives two classes dataset and dataloaders\n",
    "# Dataset and Dataloader simplifies and decouples the process of loading the data and the process of using the data to training\n",
    "# The data lies in the memory; the datasets class knows where the data lies and loads the data from the memory;\n",
    "# The dataloader class handle how to create batches, shuffling and parallel loading\n",
    "# The dataset is an abstract class; its actually a blueprint; it is defined by -\n",
    "# 1) __init__ (a constructor here) - tells data how to load the data\n",
    "# 2) __len__ (method) - returns the total number of samples ( helps to compute the total number of batches)\n",
    "# 3) __getitem__(index) - returns the data and the label at the given index\n",
    "\n",
    "# Dataloader Control Flow -\n",
    "# 1) At the start of each epoch, the Dataloader(if shuffle=True) shuffles indices(using a sampler)\n",
    "# 2) It divides the indices into clunk of batch size\n",
    "# 3) from each index in the chunk, data samples are fetched from the Dataset object using it's getitem(index) method\n",
    "# 4) The samples are then collected and combined into a batch using collate function\n",
    "# 5) The batch is returned to the training loop\n",
    "\n",
    "# The DataLoader does not compute all batches ahead of time. It creates one batch at a time on-the-fly, feeds it to the training loop, and only then proceeds to create the next batch."
   ],
   "id": "d040cdb57a42c32b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import torch"
   ],
   "id": "9b2daff972c75849",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create synthetic data\n",
    "X, y = make_classification(\n",
    "    n_samples=10,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")"
   ],
   "id": "e25187191d61e421",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X",
   "id": "85b81a1a17845826",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y",
   "id": "829a746552edc519",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ],
   "id": "d09455220a2df09f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)"
   ],
   "id": "34c700f524cc58ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from torch.utils.data import Dataset, DataLoader",
   "id": "54582f4b665d97af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# we can add transformations in the getitem fnc eg\n",
    "# resizing, b/w, augmentation for image datasets\n",
    "# or lemmatization, stopword removal, convert to lower case etc\n",
    "\n",
    "class Customdataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]"
   ],
   "id": "7477bd8533ddf258",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset = Customdataset(X,y)",
   "id": "8573b29072286447",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(dataset)",
   "id": "5650a4a53e1a84dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset[0]",
   "id": "69666342721422f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We can use parallelization to create batches in the dataloader class using \"workers\"\n",
    "# Also we can use different sampling strategy in PyTorch\n",
    "# like SequentialSampling or RandomSampling, or even Custom Sampling etc\n",
    "\n",
    "# Collate Function: The collate_fn in PyTorch's DataLoader is a function that specifies\n",
    "# how to combine a list of samples from a dataset into a single batch.\n",
    "# By default, the DataLoader uses a simple batch collation mechanism,\n",
    "# but collate_fn allows you to customize how the data should be processed and batched\n",
    "# the collate function also can be used to add padding in the dataset to match shape\n",
    "# of tensors such that they could be stacked\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False) # dataloader is an iterable"
   ],
   "id": "125ccf1d9a60fd0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for batch_features, batch_labels in dataloader:\n",
    "    print(batch_features)\n",
    "    print(batch_labels)\n",
    "    print(\"-\"*40)"
   ],
   "id": "c7576532db93092f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Implementing mini batch gradient descent in our dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ],
   "id": "bee90582c0c05964",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
    "df.head()"
   ],
   "id": "2396a72f17d38379",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove unwanted columns\n",
    "\n",
    "df = df.drop(columns=['id','Unnamed: 32'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:], df.iloc[:,0], test_size=0.2)\n",
    "\n",
    "# Scaling\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.transform(X_test)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test_tensor = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train_tensor = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test_tensor = torch.from_numpy(y_test.astype(np.float32))"
   ],
   "id": "3acac31be2727b00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# defining params\n",
    "\n",
    "lr = 0.1\n",
    "epochs = 25"
   ],
   "id": "e6cca3bb131380cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ],
   "id": "2bdfeb13ed20daec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = CustomDataset(X_test_tensor, y_test_tensor)"
   ],
   "id": "f69f6328e8570cf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ],
   "id": "459ae3d3c36954ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Defining the model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, features):\n",
    "        out = self.linear(features)\n",
    "        return self.sigmoid(out)\n"
   ],
   "id": "309f8891e0cbd79c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = Model(X_train_tensor.shape[1])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "loss_fnc = nn.BCELoss()"
   ],
   "id": "5517418c833d5009",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training Pipeline\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_features, batch_labels in train_loader:\n",
    "        y_pred = model(batch_features)\n",
    "        loss = loss_fnc(y_pred, batch_labels.view(-1,1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch +1}; Loss: {loss.item()}')"
   ],
   "id": "b1bf63602bad755b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluation\n",
    "\n",
    "model.eval() # Set the model to evaluation mode\n",
    "\n",
    "accuracy_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_labels in test_loader:\n",
    "        # Forward pass\n",
    "        y_pred = model(batch_features)\n",
    "        y_pred = (y_pred > 0.8).float()\n",
    "        # Calculate accuracy for the current batch\n",
    "        batch_accuracy = (y_pred.view(-1) == batch_labels).float().mean().item()\n",
    "        print(batch_accuracy)\n",
    "        accuracy_list.append(batch_accuracy)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
    "print(f'Accuracy: {overall_accuracy:.4f}')"
   ],
   "id": "807fbd3640bb3b29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Building an ANN on fashion MNIST data\n",
    "#### The ANN will have an input layers with 784 nodes as there are 784 different features\n",
    "#### then we will have 2 hidden layers\n",
    "#### 1st hidden layer with 128 neurons (with relu)\n",
    "#### 2nd hidden layer with 64 neurons (with relu)\n",
    "#### output layer with 10 neurons as there are 10 classes (with softmax activation because of multi-class classification problem)\n",
    "\n",
    "##### Workflow: 1) Dataloader objects -> 2) Training loop -> 3) Evaluation"
   ],
   "id": "72c6e740884c69fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "id": "34d1050d44b95629",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.manual_seed(42)",
   "id": "f8f68857168335c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('/home/ml02/Downloads/PyTorch/fmnist_small.csv')\n",
    "df.head()\n",
    "\n",
    "# The dataset has 785 columns as the first column is the label and the rest 784 columns are flattened value of every pixel\n",
    "# of our 28x28 resolution image"
   ],
   "id": "39af9feff088fe4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating grid to visualize image\n",
    "\n",
    "fig, axes = plt.subplots(4,4, figsize=(10,10))\n",
    "fig.suptitle(\"First 16 images\", fontsize = 12)\n",
    "\n",
    "# Plotting\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    img = df.iloc[i,1:].values.reshape(28,28) # reshaping to the original image size\n",
    "    ax.imshow(img) # Will display in grayscale\n",
    "    ax.axis('off') # Removes the axis\n",
    "    ax.set_title(f\"Label: {df.iloc[i,0]}\")\n",
    "plt.tight_layout(rect=[0,0,1,0.96])\n",
    "plt.show()"
   ],
   "id": "5b5a72428ea2041",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train_test_split\n",
    "\n",
    "X = df.iloc[:,1:].values  # .values converts the pandas dataframe to numpy array\n",
    "y = df.iloc[:,0].values\n",
    "type(X)"
   ],
   "id": "6a97907d081c1e79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
   "id": "cb0b79eb16ff3d85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Scaling the features\n",
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0"
   ],
   "id": "7be21fbad1e33173",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating Custom dataset Class\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32) # Reminder pytorch wants to see the features in float dtype\n",
    "        # and labels in long dtype (in classification)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.X) # or return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ],
   "id": "c06e32a2bb5f4ee0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create train_dataset object\n",
    "train_dataset = CustomDataset(X_train, y_train)"
   ],
   "id": "3c70bac5ba31819d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(train_dataset)",
   "id": "c35678fd4a6217f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_dataset[0]",
   "id": "4a72bb29b533fd76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create test_dataset object\n",
    "test_dataset = CustomDataset(X_test, y_test)"
   ],
   "id": "56c7405323e0222f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(test_dataset)",
   "id": "8b1ebf7bc8e16576",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_dataset[0]",
   "id": "1a6a6721e15c6d4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating  train and test loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# During prediction, we don't want to shuffle data because if we do shuffle in prediction data\n",
    "# it will be hard to calculate accuracy because to calculate accuracy\n",
    "# we want to infer at each datapoint which will be problematic if shuffle is true"
   ],
   "id": "e8a53ab788dcef5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# total number of train batches\n",
    "len(train_loader)"
   ],
   "id": "d964288ba3bb5803",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# total number of test batches\n",
    "len(test_loader)"
   ],
   "id": "e0d2f79dfff0a9a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define nn class(cpu)\n",
    "\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(num_features, 128), # 1st Hidden Layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64), # 2nd Hidden Layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,10) # Output Layer\n",
    "            # If we were building this nn block in tensorflow\n",
    "            # then we need to specific another layer i.e. softmax\n",
    "            # but in PyTorch there is CrossEntropyLoss\n",
    "            # which has softmax implemented in it by default\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "id": "6b522b3dba5f7dbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# set the learning rate and epochs\n",
    "epochs = 100\n",
    "lr = 0.1"
   ],
   "id": "838ae0dc074cd78e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(X_train.shape)\n",
    "print(X_train.shape[1])"
   ],
   "id": "5354be552eafe507",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# instantiate the model\n",
    "model = ANNModel(X_train.shape[1])\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ],
   "id": "94e17cb3edda672c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_epoch_loss = 0\n",
    "    for features_batch, labels_batch in train_loader:\n",
    "        # forward pass\n",
    "        out = model(features_batch)\n",
    "        # loss calculation\n",
    "        loss = criterion(out, labels_batch)\n",
    "        # Clear gradients before backward pass\n",
    "        optimizer.zero_grad()\n",
    "        # back pass\n",
    "        loss.backward()\n",
    "        # update grads\n",
    "        optimizer.step()\n",
    "        total_epoch_loss += loss.item()\n",
    "    # print average loss in every epoch\n",
    "    average_loss = total_epoch_loss/len(train_loader)\n",
    "    print(f'Epoch:{epoch+1}, Avg. Loss:{average_loss:.4f}')\n",
    "\n",
    "print(f'Total Loss:{total_epoch_loss:.4f}')"
   ],
   "id": "9f539b04b6a6b4f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluation code\n",
    "\n",
    "# 1st set the model in evaluation mode\n",
    "# Why do we need to use model.eval in the first place??\n",
    "# The answer is - As in the training process there is dropout implemented,\n",
    "# which turn's off few neurons randomly while training to prevent overfitting;\n",
    "# but while prediction we want to use all the available neurons\n",
    "# Another example could be batch_normalization - this calculates mean and\n",
    "# standard_deviation by looking at the training data\n",
    "# but during the time of prediction we want to use the mean and std. dev.\n",
    "# which was calculated during the training only\n",
    "##  Here in our code sofar we have not used dropout or batch normalization\n",
    "# but setting the model to eval mode during evaluation is a good practice\n",
    "\n",
    "model.eval()\n"
   ],
   "id": "8f468f61413d470b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# custom evaluation code\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# Don't want to calculate gradient during prediction\n",
    "with torch.no_grad():\n",
    "    # We will loop though the test data and will make prediction to see if it is correct or wrong\n",
    "    for batch_feature, batch_labels in test_loader:\n",
    "        # to predict we will run forward pass\n",
    "        outputs = model(batch_feature)\n",
    "        # let's understand how the output will be shaped like:\n",
    "        # Evert batch is having 32 images here, and as we have 10 classes\n",
    "        # the output will have those 10 different probability for each 10 classes for each 32 images\n",
    "        # so the output will be shaped like a 32x10 matrix\n",
    "        # to extract which label the image belongs to we will select the position of the class with the max probability\n",
    "        # the extracted labels will be of [32] only\n",
    "        _, predicted = torch.max(outputs, 1) # Calculates the max value and their corresponding indices\n",
    "        # along dimension 1 i.e. column of the output tensor\n",
    "        # '_' ignores the maximum values here, predicted stores the indices of the highest score for each image\n",
    "        total += batch_labels.shape[0] # Adds 32 for every batch\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "print(f'Accuracy is: {correct/total:0.4f}')"
   ],
   "id": "f6fe2522595e6d33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### training in GPU",
   "id": "4a823ef078bfc8be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "3e6e00d0a2479627",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.read_csv('/home/ml02/Downloads/PyTorch/archive/fashion-mnist_train.csv')",
   "id": "cf4fd6551400933a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "f4cb79f134a2bb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Step 1 - Check GPU availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "id": "32994e1ff1824130",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We need to pass .values because we can't send the column names in the training dataset just the values\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:].values, df.iloc[:,0].values, test_size=0.2)"
   ],
   "id": "bddc594c1ad8f495",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0"
   ],
   "id": "f98a07863735710d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ],
   "id": "ea769d6741c74c3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)"
   ],
   "id": "a4a9ea7d246e323d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ],
   "id": "baa329d0bed7a782",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Note: to optimize the GPU usage we can also do -\n",
    "# a) Increase Batch Size - Better Utilize GPU memory and reduce computation time per epoch\n",
    "# b) Enable Dataloader Pinning- Use pin_memory=True to speed up data transfer from CPU to GPU\n",
    "# Eg. -\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
    "\n",
    "# NB - https://discuss.pytorch.org/t/should-i-turn-off-pin-memory-when-i-already-loaded-the-image-to-the-gpu-in-getitem/166276\n",
    "# Turn off pin_memory if tensor is already loaded into GPU, else initialise tensor in CPU and then do pin_memory on it"
   ],
   "id": "a7b2a38222ef0320",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(len(train_loader)) # print no of train batches\n",
    "print(len(test_loader)) # print no of test batches"
   ],
   "id": "c7e90c6319e1c5b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MyNN(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "id": "5bdcedfc2ebe3886",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lr = 0.1\n",
    "epochs = 100"
   ],
   "id": "6de09aa2b03c5852",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PyTorch doesn't automatically assume that just because your data is on GPU , your model should be too .\n",
    "#) Models and data must be on the same device for computation to work.\n",
    "# even if you create your tensors on a specific device eg 'cuda';\n",
    "# you should explicitly do model.to('cuda') to use gpu\n",
    "\n",
    "model = MyNN(X_train.shape[1])\n",
    "model.to(device) ## Step 2 - Moving the model to gpu\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=lr)"
   ],
   "id": "eeb559478ab653a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for epoch in range(epochs):\n",
    "    total_epoch_loss = 0\n",
    "    for batch_features, batch_labels in train_loader:\n",
    "        # Step 3 - Move train data to GPU\n",
    "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_epoch_loss += loss.item()\n",
    "    avg_loss = total_epoch_loss/len(train_loader)\n",
    "    print(f'Epoch: {epoch+1}; Avg. Loss: {avg_loss:.4f}')"
   ],
   "id": "50901af70a7b17ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.eval()",
   "id": "c48c5b97c9d9a639",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_labels in test_loader:\n",
    "        # Step 4 - Move test data to GPU\n",
    "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "        outputs = model(batch_features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_labels.shape[0]\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "print(f'Testing Accuracy: {correct/total:.4f}')"
   ],
   "id": "37c01d076b00db8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_labels in train_loader:\n",
    "        # Step 4 - Move test data to GPU\n",
    "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "        outputs = model(batch_features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_labels.shape[0]\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "print(f'Training Accuracy: {correct/total:.4f}')"
   ],
   "id": "fc8752384281c828",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model is overfitted as Training Accu is 97% and test accu is 88%\n",
    "\n",
    "# Solutions to reduce overfitting:\n",
    "\n",
    "# 1. Adding more data; 2. Reducing the complexity of NN architecture;\n",
    "# 3. Regularization(Adding penalty term in loss fnc and try to reduce both);\n",
    "# 4. Dropouts; 5. Data Augmentation; 6. Early Stopping\n",
    "# 7. Batch Normalization(Used to stabilise training which comes with an effect of regularization);\n",
    "\n",
    "# Note about Dropout :-\n",
    "# 1. Applied to the hidden layers only\n",
    "# 2. Applied after activation function\n",
    "# 3. During each forward pass it randomly turns off p% neurons in the hidden layers\n",
    "# 4. This has a regularization effect as basically during each forward\n",
    "# pass we are technically using different structured Neural Networks\n",
    "# 5. During evaluation Dropout is not used\n",
    "\n",
    "# Note about BatchNormalization:\n",
    "# 1. Typically applied to the hidden layers of a neural network, but not to the output layer.\n",
    "# 2. Applied after Linear layers and Before Activation Fncs. -\n",
    "# Normalizes the output of the preceding layer (e.g. - after nn.Linear)\n",
    "# and is usually followed by an activation function e.g. ReLU\n",
    "# 3. It actually computed the mean and variance of the activations within a mini-batch\n",
    "# and uses these statistics to normalize the activations\n",
    "# 4. Introduces learnable parameters - gamma(scaling) and beta(shifting)\n",
    "# allowing the network to adjust the normalized outputs\n",
    "# 5. Improves training stability by reducing internal covariate shift,\n",
    "# stabilizing the training process and allowing the use of higher learning rates.\n",
    "# 6. Introduces regularization effect as the statistics computed over a mini-batch\n",
    "# adds noise to the training process.\n",
    "# 7. During evaluation, BatchNorm uses the running mean and variance accumulated during training,\n",
    "# rather than recomputing them from the mini batch(reduces data leakage and not touching test dataset)"
   ],
   "id": "d293f9049370c379",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ***Note About L2 regularization***\n",
    "\n",
    "Neural Networks are basically optimization problems which has a loss fnc like MSE in regression, logloss for classifications etc. The main goal is to find such values of the parameters of our model i.e. of weights and biases such that the loss is minimum. It is applied to model weights to penalize large values and encounter smaller more generalizable weights.\n",
    "In the case of regularization we add an additional penalty term with the loss function so that we minimize the combination of the loss and as well as the penalty term and by doing so, high values of weights are not attained which reduces the possibility of overfitting.\n",
    "\n",
    "$$ \\text{L2 regularization:\\quad}\n",
    "\\text{Loss}_{\\text{reg}} = \\text{Loss}_{\\text{Original}} + \\lambda\\Sigma w^2_i\n",
    "$$\n",
    "The $\\lambda$ is called the regularization coefft., which is set as peer need as how strong we need the regularization effect.\n",
    "In Weight decay, directly modifies the gradient update rule to include $\\lambda w_i$, effectively shrinking weights during training. This can be done in pytorch directly in the optimization step. We are directly adding loss with gradient here.\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta(\\Delta\\text{Loss}+\\lambda w)\n",
    "$$\n",
    "\n",
    "**Penalizes Large Weights:** During the training process, in the neural network, those weights whom have big values, are being penalized and those weights are distributed into other weights, so that the over-reliance on a particular weight becomes less and weight balancing is maintained in the network.\n",
    "\n",
    "**No Effect on Bias Terms:** As biases don't directly affect model complexity, regularization is only typically applied only on weights not on bias.\n",
    "\n",
    "Regularization affects weights updates only during training, and should not be used in inference/prediction."
   ],
   "id": "5d4fc09d00d3aa54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Optimizing the network",
   "id": "ae3691a2924c6a70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "db7cc025f023e856",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.read_csv('/home/ml02/Downloads/PyTorch/archive/fashion-mnist_train.csv')",
   "id": "34e3fb6ea0b9e275",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "559dee6ca4a11253",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "device = 'cuda' if torch.cuda.is_available else 'cpu'",
   "id": "b22c9b22f3ddfb04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:].values, df.iloc[:,0].values, test_size=0.2)",
   "id": "abffd8ba5a0410fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0"
   ],
   "id": "e8641e1e61b6b954",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CustomDataset(Dataset): # Using polymorphism and abstraction both\n",
    "    def __init__(self, features, labels):\n",
    "        # You need to give the features as tensors in the dataset\n",
    "        self.features = torch.tensor(features, dtype=torch.float32,device=device)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long,device=device)\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, item):\n",
    "        return self.features[item], self.labels[item]"
   ],
   "id": "2551f217a89ff729",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# You need to make the dataset now\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)"
   ],
   "id": "120a510ef1bd7502",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 128, shuffle=True)"
   ],
   "id": "43a721416e305226",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(num_features,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(128,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(64,10)\n",
    "        )\n",
    "    def forward(self, features):\n",
    "        return self.model(features)"
   ],
   "id": "cab39fe54db6023e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lr = 0.1\n",
    "epochs = 100"
   ],
   "id": "28b4dfe4b7a53748",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = MyModel(X_train.shape[1])\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# weight_decay is our L2 Regularization coefft.\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=1e-4)"
   ],
   "id": "54c0241d50af13a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for epoch in range(epochs):\n",
    "    # Put the model in training mode\n",
    "    model.train()\n",
    "    total_epoch_loss = 0\n",
    "    for batch_features, batch_labels in train_loader:\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_epoch_loss += loss.item()\n",
    "    print(f'Epoch: {epoch+1}; Avg. Loss: {total_epoch_loss/len(train_loader):.4f}')"
   ],
   "id": "3a735d29cc75cd3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.eval()",
   "id": "aa85c285181ba2e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_labels in test_loader:\n",
    "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "        outputs = model(batch_features)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        total += batch_labels.shape[0]\n",
    "        correct += (predictions == batch_labels).sum().item()\n",
    "\n",
    "print(f'Testing Accuracy: {correct/total:.4f}')"
   ],
   "id": "2c9ade80764e8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_labels in train_loader:\n",
    "        # Step 4 - Move test data to GPU\n",
    "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "        outputs = model(batch_features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_labels.shape[0]\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "print(f'Training Accuracy: {correct/total:.4f}')"
   ],
   "id": "3b6ad81b661e4b7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from torchvision import models",
   "id": "31f33bd6e73a4a07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dir(models)",
   "id": "3f3e95af332793ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Terms in Optuna\n",
    "\n",
    "1. Study: A Study in Optuna is an optimization session that encompasses multiple trails. Essential objective is to optimize the objective fnc; i.e. the training and  finding the best hyperparameters are called as study in term's of Optuna's language.\n",
    "2. Trial: A trial is a single iteration of the optimization process where a specific set of hyperparameters is evaluated. Each trial runs the obj func once with a distinct set of hyperparameters.\n",
    "3. Trial Parameters: Specific hyperparameters values chosen during a trial. Each trial will have a unique combination of hyperparameters that are evaluated to see how they impact the objective function. E.g. - Different different learning rates, batch_sizes during each trial.\n",
    "4. Objective Function: The fnc to be optimized(minimized or maximized) during the hyperparameter search. It takes hyperparameters as input and return a value (eg accuracy, loss, etc) that Optuna tries to optimize. E.g. - In a classification tack, the obj fnc could be the cross-entropy loss which Optuna seeks to optimize.\n",
    "5. Sampler: A sampler is the algorithm that suggests which hyperparameters should be evaluated next. Optuna uses the Tree-structured Parzen Estimator(TPE) by default(TPE uses Bayesian Optimization), but can also support other sampling like Random Sampling or even Custom Sampling,"
   ],
   "id": "2e8b682896ddee0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T14:55:08.485673Z",
     "start_time": "2025-07-01T14:55:08.482775Z"
    }
   },
   "cell_type": "code",
   "source": "# !uv pip install optuna",
   "id": "ddaf11ac6ab742ec",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T12:39:26.076688Z",
     "start_time": "2025-07-02T12:39:24.886278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI',\n",
    "           'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "\n",
    "df = pd.read_csv(url, names=columns)\n",
    "df.head()"
   ],
   "id": "c96781a51f2fe5dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T12:39:26.125458Z",
     "start_time": "2025-07-02T12:39:26.116585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Missing Value Imputation\n",
    "cols_with_missing_vals = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "df[cols_with_missing_vals] = df[cols_with_missing_vals].replace(0,np.nan)\n",
    "\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "print(df.isnull().sum())"
   ],
   "id": "c7c105d30ba6dab4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies                 0\n",
      "Glucose                     0\n",
      "BloodPressure               0\n",
      "SkinThickness               0\n",
      "Insulin                     0\n",
      "BMI                         0\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "Outcome                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T12:39:27.906256Z",
     "start_time": "2025-07-02T12:39:27.897821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42)\n",
    "\n",
    "# Optional Scaling\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.transform(X_test)\n",
    "\n",
    "print(f'Training set shape: {X_train.shape}')\n",
    "print(f'Test set shape: {X_test.shape}')"
   ],
   "id": "adae445ce3e8d8c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (537, 8)\n",
      "Test set shape: (231, 8)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T12:39:31.853978Z",
     "start_time": "2025-07-02T12:39:31.850955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we will define an objective function for the workflow of optuna\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "# Define the objective fnc\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest values of the hyperparameters\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "    # Here the search space will be in the Range [50,200] for n_estimators\n",
    "    # and range of [3,20] for max_depth\n",
    "\n",
    "    # Create the RandomForestClassifier with suggested hyperparameters\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42\n",
    "    )\n",
    "    # Perform 3-fold cross-validation and calculate accuracy\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "    return score"
   ],
   "id": "c0dc85254e775973",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T12:40:04.757660Z",
     "start_time": "2025-07-02T12:39:34.174367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "# As we are trying to 'Maximize' our accuracy so our direction of the objective function will be to maximize it\n",
    "study.optimize(objective, n_trials=100) # Running 50 trials to find the optimal hyperparameters\n",
    "\n",
    "# Print the best result\n",
    "print(f'Best trial accuracy: {study.best_trial.value}')\n",
    "print(f'Best trial Hyperparameters: {study.best_trial.params}')\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a RandomForestClassifier using the best hyperparameters from Optuna\n",
    "best_model = RandomForestClassifier(**study.best_trial.params, random_state=42)\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Test accuracy ith best hyperparameters: {test_accuracy:.2f}')"
   ],
   "id": "49d09e4ef0554a00",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-02 18:09:34,177] A new study created in memory with name: no-name-ff348ff6-4448-4c70-8572-50bcfebb1d4f\n",
      "[I 2025-07-02 18:09:34,487] Trial 0 finished with value: 0.7783985102420856 and parameters: {'n_estimators': 106, 'max_depth': 18}. Best is trial 0 with value: 0.7783985102420856.\n",
      "[I 2025-07-02 18:09:34,709] Trial 1 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 59, 'max_depth': 13}. Best is trial 0 with value: 0.7783985102420856.\n",
      "[I 2025-07-02 18:09:35,257] Trial 2 finished with value: 0.756052141527002 and parameters: {'n_estimators': 173, 'max_depth': 4}. Best is trial 0 with value: 0.7783985102420856.\n",
      "[I 2025-07-02 18:09:35,666] Trial 3 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 130, 'max_depth': 11}. Best is trial 0 with value: 0.7783985102420856.\n",
      "[I 2025-07-02 18:09:36,264] Trial 4 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 200, 'max_depth': 10}. Best is trial 0 with value: 0.7783985102420856.\n",
      "[I 2025-07-02 18:09:36,613] Trial 5 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 102, 'max_depth': 11}. Best is trial 0 with value: 0.7783985102420856.\n",
      "[I 2025-07-02 18:09:37,068] Trial 6 finished with value: 0.7597765363128491 and parameters: {'n_estimators': 148, 'max_depth': 6}. Best is trial 0 with value: 0.7783985102420856.\n",
      "[I 2025-07-02 18:09:37,541] Trial 7 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 156, 'max_depth': 15}. Best is trial 0 with value: 0.7783985102420856.\n",
      "[I 2025-07-02 18:09:38,147] Trial 8 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 169, 'max_depth': 12}. Best is trial 0 with value: 0.7783985102420856.\n",
      "[I 2025-07-02 18:09:38,474] Trial 9 finished with value: 0.7597765363128491 and parameters: {'n_estimators': 111, 'max_depth': 3}. Best is trial 0 with value: 0.7783985102420856.\n",
      "[I 2025-07-02 18:09:38,730] Trial 10 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 71, 'max_depth': 19}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:38,986] Trial 11 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 70, 'max_depth': 20}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:39,286] Trial 12 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 82, 'max_depth': 20}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:39,624] Trial 13 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 91, 'max_depth': 17}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:40,029] Trial 14 finished with value: 0.7783985102420856 and parameters: {'n_estimators': 119, 'max_depth': 17}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:40,184] Trial 15 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 52, 'max_depth': 18}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:40,427] Trial 16 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 81, 'max_depth': 15}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:40,844] Trial 17 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 131, 'max_depth': 8}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:41,211] Trial 18 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 99, 'max_depth': 15}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:41,441] Trial 19 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 67, 'max_depth': 18}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:41,697] Trial 20 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 72, 'max_depth': 20}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:41,991] Trial 21 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 68, 'max_depth': 20}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:42,152] Trial 22 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 50, 'max_depth': 18}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:42,419] Trial 23 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 71, 'max_depth': 16}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:42,709] Trial 24 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 81, 'max_depth': 19}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:42,937] Trial 25 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 63, 'max_depth': 14}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:43,278] Trial 26 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 91, 'max_depth': 19}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:43,574] Trial 27 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 78, 'max_depth': 17}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:43,921] Trial 28 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 89, 'max_depth': 19}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:44,263] Trial 29 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 111, 'max_depth': 18}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:44,440] Trial 30 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 60, 'max_depth': 16}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:44,617] Trial 31 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 58, 'max_depth': 20}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:44,945] Trial 32 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 97, 'max_depth': 13}. Best is trial 10 with value: 0.7783985102420857.\n",
      "[I 2025-07-02 18:09:45,181] Trial 33 finished with value: 0.7839851024208566 and parameters: {'n_estimators': 73, 'max_depth': 18}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:45,407] Trial 34 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 72, 'max_depth': 19}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:45,595] Trial 35 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 60, 'max_depth': 16}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:45,845] Trial 36 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 77, 'max_depth': 17}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:46,524] Trial 37 finished with value: 0.7765363128491619 and parameters: {'n_estimators': 198, 'max_depth': 18}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:46,833] Trial 38 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 90, 'max_depth': 14}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:47,370] Trial 39 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 138, 'max_depth': 8}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:47,587] Trial 40 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 65, 'max_depth': 19}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:47,850] Trial 41 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 71, 'max_depth': 19}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:48,129] Trial 42 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 75, 'max_depth': 20}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:48,338] Trial 43 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 56, 'max_depth': 18}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:48,677] Trial 44 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 105, 'max_depth': 19}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:48,876] Trial 45 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 67, 'max_depth': 17}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:49,149] Trial 46 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 85, 'max_depth': 20}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:49,414] Trial 47 finished with value: 0.7597765363128492 and parameters: {'n_estimators': 95, 'max_depth': 10}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:49,639] Trial 48 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 74, 'max_depth': 19}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:50,043] Trial 49 finished with value: 0.7802607076350093 and parameters: {'n_estimators': 115, 'max_depth': 16}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:50,825] Trial 50 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 156, 'max_depth': 16}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:51,207] Trial 51 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 120, 'max_depth': 18}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:51,865] Trial 52 finished with value: 0.7765363128491619 and parameters: {'n_estimators': 171, 'max_depth': 17}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:52,137] Trial 53 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 85, 'max_depth': 20}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:52,309] Trial 54 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 53, 'max_depth': 5}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:53,050] Trial 55 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 182, 'max_depth': 18}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:53,269] Trial 56 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 64, 'max_depth': 15}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:53,606] Trial 57 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 108, 'max_depth': 20}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:53,861] Trial 58 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 83, 'max_depth': 17}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:54,347] Trial 59 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 139, 'max_depth': 12}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:54,633] Trial 60 finished with value: 0.7821229050279331 and parameters: {'n_estimators': 73, 'max_depth': 19}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:54,898] Trial 61 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 71, 'max_depth': 19}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:55,150] Trial 62 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 78, 'max_depth': 19}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:55,320] Trial 63 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 50, 'max_depth': 18}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:55,528] Trial 64 finished with value: 0.7821229050279331 and parameters: {'n_estimators': 63, 'max_depth': 16}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:55,740] Trial 65 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 56, 'max_depth': 14}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:55,968] Trial 66 finished with value: 0.7821229050279331 and parameters: {'n_estimators': 63, 'max_depth': 16}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:56,201] Trial 67 finished with value: 0.7821229050279331 and parameters: {'n_estimators': 63, 'max_depth': 16}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:56,400] Trial 68 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 63, 'max_depth': 15}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:56,580] Trial 69 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 59, 'max_depth': 16}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:56,784] Trial 70 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 67, 'max_depth': 13}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:56,976] Trial 71 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 61, 'max_depth': 16}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:57,183] Trial 72 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 67, 'max_depth': 15}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:57,347] Trial 73 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 55, 'max_depth': 14}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:57,621] Trial 74 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 78, 'max_depth': 17}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:57,868] Trial 75 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 88, 'max_depth': 16}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:58,221] Trial 76 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 116, 'max_depth': 17}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:58,510] Trial 77 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 94, 'max_depth': 15}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:58,735] Trial 78 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 75, 'max_depth': 16}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:58,977] Trial 79 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 81, 'max_depth': 18}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:59,186] Trial 80 finished with value: 0.7802607076350094 and parameters: {'n_estimators': 70, 'max_depth': 17}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:59,422] Trial 81 finished with value: 0.7802607076350094 and parameters: {'n_estimators': 70, 'max_depth': 17}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:59,682] Trial 82 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 69, 'max_depth': 17}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:09:59,917] Trial 83 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 63, 'max_depth': 18}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:10:00,132] Trial 84 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 73, 'max_depth': 16}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:10:00,300] Trial 85 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 53, 'max_depth': 15}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:10:00,506] Trial 86 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 57, 'max_depth': 17}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:10:00,768] Trial 87 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 87, 'max_depth': 17}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:10:01,064] Trial 88 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 101, 'max_depth': 14}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:10:01,270] Trial 89 finished with value: 0.7541899441340782 and parameters: {'n_estimators': 80, 'max_depth': 3}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:10:01,477] Trial 90 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 61, 'max_depth': 13}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:10:01,728] Trial 91 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 69, 'max_depth': 18}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:10:01,955] Trial 92 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 75, 'max_depth': 18}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:10:02,154] Trial 93 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 66, 'max_depth': 16}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:10:02,412] Trial 94 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 71, 'max_depth': 19}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:10:02,782] Trial 95 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 128, 'max_depth': 8}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:10:03,045] Trial 96 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 64, 'max_depth': 18}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:10:03,411] Trial 97 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 84, 'max_depth': 7}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:10:03,728] Trial 98 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 77, 'max_depth': 17}. Best is trial 33 with value: 0.7839851024208566.\n",
      "[I 2025-07-02 18:10:04,581] Trial 99 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 160, 'max_depth': 19}. Best is trial 33 with value: 0.7839851024208566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial accuracy: 0.7839851024208566\n",
      "Best trial Hyperparameters: {'n_estimators': 73, 'max_depth': 18}\n",
      "Test accuracy ith best hyperparameters: 0.76\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T12:32:59.172106Z",
     "start_time": "2025-07-02T12:31:46.896005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we will use RandomSampler\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.RandomSampler())\n",
    "# As we are trying to 'Maximize' our accuracy so our direction of the objective function will be to maximize it\n",
    "study.optimize(objective, n_trials=100) # Running 50 trials to find the optimal hyperparameters\n",
    "\n",
    "# Print the best result\n",
    "print(f'Best trial accuracy: {study.best_trial.value}')\n",
    "print(f'Best trial Hyperparameters: {study.best_trial.params}')\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a RandomForestClassifier using the best hyperparameters from Optuna\n",
    "best_model = RandomForestClassifier(**study.best_trial.params, random_state=42)\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Test accuracy ith best hyperparameters: {test_accuracy:.2f}')"
   ],
   "id": "399d8e68401a5209",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-02 18:01:46,900] A new study created in memory with name: no-name-0a892328-555f-46ba-a887-a6036f217a80\n",
      "[I 2025-07-02 18:01:47,366] Trial 0 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 109, 'max_depth': 5}. Best is trial 0 with value: 0.7672253258845437.\n",
      "[I 2025-07-02 18:01:47,665] Trial 1 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 72, 'max_depth': 5}. Best is trial 1 with value: 0.7746741154562384.\n",
      "[I 2025-07-02 18:01:48,205] Trial 2 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 116, 'max_depth': 8}. Best is trial 1 with value: 0.7746741154562384.\n",
      "[I 2025-07-02 18:01:48,474] Trial 3 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 58, 'max_depth': 8}. Best is trial 1 with value: 0.7746741154562384.\n",
      "[I 2025-07-02 18:01:48,933] Trial 4 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 98, 'max_depth': 14}. Best is trial 1 with value: 0.7746741154562384.\n",
      "[I 2025-07-02 18:01:49,611] Trial 5 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 144, 'max_depth': 5}. Best is trial 1 with value: 0.7746741154562384.\n",
      "[I 2025-07-02 18:01:50,077] Trial 6 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 92, 'max_depth': 5}. Best is trial 1 with value: 0.7746741154562384.\n",
      "[I 2025-07-02 18:01:50,846] Trial 7 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 166, 'max_depth': 5}. Best is trial 1 with value: 0.7746741154562384.\n",
      "[I 2025-07-02 18:01:51,216] Trial 8 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 92, 'max_depth': 15}. Best is trial 1 with value: 0.7746741154562384.\n",
      "[I 2025-07-02 18:01:51,841] Trial 9 finished with value: 0.7802607076350093 and parameters: {'n_estimators': 133, 'max_depth': 18}. Best is trial 9 with value: 0.7802607076350093.\n",
      "[I 2025-07-02 18:01:52,318] Trial 10 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 117, 'max_depth': 5}. Best is trial 9 with value: 0.7802607076350093.\n",
      "[I 2025-07-02 18:01:52,898] Trial 11 finished with value: 0.7858472998137803 and parameters: {'n_estimators': 121, 'max_depth': 15}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:01:53,452] Trial 12 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 130, 'max_depth': 20}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:01:53,943] Trial 13 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 101, 'max_depth': 7}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:01:54,365] Trial 14 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 105, 'max_depth': 13}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:01:55,331] Trial 15 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 191, 'max_depth': 5}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:01:55,683] Trial 16 finished with value: 0.7821229050279331 and parameters: {'n_estimators': 73, 'max_depth': 20}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:01:56,188] Trial 17 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 100, 'max_depth': 14}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:01:56,932] Trial 18 finished with value: 0.7765363128491619 and parameters: {'n_estimators': 134, 'max_depth': 19}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:01:57,834] Trial 19 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 150, 'max_depth': 11}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:01:58,252] Trial 20 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 82, 'max_depth': 8}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:01:58,955] Trial 21 finished with value: 0.7765363128491619 and parameters: {'n_estimators': 135, 'max_depth': 17}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:01:59,756] Trial 22 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 95, 'max_depth': 11}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:00,425] Trial 23 finished with value: 0.7597765363128491 and parameters: {'n_estimators': 118, 'max_depth': 11}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:01,437] Trial 24 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 174, 'max_depth': 6}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:02,495] Trial 25 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 166, 'max_depth': 17}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:03,099] Trial 26 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 118, 'max_depth': 12}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:03,986] Trial 27 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 132, 'max_depth': 18}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:05,080] Trial 28 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 160, 'max_depth': 20}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:05,967] Trial 29 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 123, 'max_depth': 14}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:06,374] Trial 30 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 71, 'max_depth': 12}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:07,401] Trial 31 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 174, 'max_depth': 6}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:08,149] Trial 32 finished with value: 0.7821229050279329 and parameters: {'n_estimators': 120, 'max_depth': 7}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:09,212] Trial 33 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 161, 'max_depth': 12}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:09,480] Trial 34 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 53, 'max_depth': 16}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:10,976] Trial 35 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 129, 'max_depth': 15}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:12,339] Trial 36 finished with value: 0.7783985102420856 and parameters: {'n_estimators': 193, 'max_depth': 16}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:14,259] Trial 37 finished with value: 0.7616387337057727 and parameters: {'n_estimators': 170, 'max_depth': 9}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:14,724] Trial 38 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 77, 'max_depth': 8}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:15,854] Trial 39 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 163, 'max_depth': 19}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:16,382] Trial 40 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 66, 'max_depth': 14}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:16,724] Trial 41 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 67, 'max_depth': 6}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:17,053] Trial 42 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 62, 'max_depth': 11}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:18,286] Trial 43 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 182, 'max_depth': 7}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:19,861] Trial 44 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 164, 'max_depth': 7}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:20,831] Trial 45 finished with value: 0.7541899441340782 and parameters: {'n_estimators': 161, 'max_depth': 3}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:21,118] Trial 46 finished with value: 0.7523277467411545 and parameters: {'n_estimators': 53, 'max_depth': 10}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:21,641] Trial 47 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 95, 'max_depth': 13}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:22,492] Trial 48 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 118, 'max_depth': 10}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:23,662] Trial 49 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 172, 'max_depth': 20}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:24,149] Trial 50 finished with value: 0.7541899441340782 and parameters: {'n_estimators': 94, 'max_depth': 3}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:25,471] Trial 51 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 187, 'max_depth': 7}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:27,067] Trial 52 finished with value: 0.7616387337057727 and parameters: {'n_estimators': 185, 'max_depth': 5}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:28,429] Trial 53 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 188, 'max_depth': 14}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:29,007] Trial 54 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 93, 'max_depth': 5}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:30,848] Trial 55 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 190, 'max_depth': 20}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:32,175] Trial 56 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 186, 'max_depth': 15}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:32,591] Trial 57 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 68, 'max_depth': 11}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:32,912] Trial 58 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 52, 'max_depth': 13}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:34,259] Trial 59 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 165, 'max_depth': 17}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:34,685] Trial 60 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 82, 'max_depth': 5}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:35,487] Trial 61 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 132, 'max_depth': 18}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:36,188] Trial 62 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 91, 'max_depth': 11}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:36,753] Trial 63 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 89, 'max_depth': 17}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:37,044] Trial 64 finished with value: 0.7616387337057727 and parameters: {'n_estimators': 52, 'max_depth': 10}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:37,795] Trial 65 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 133, 'max_depth': 9}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:38,230] Trial 66 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 69, 'max_depth': 4}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:38,848] Trial 67 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 111, 'max_depth': 20}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:39,967] Trial 68 finished with value: 0.7597765363128491 and parameters: {'n_estimators': 196, 'max_depth': 6}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:40,939] Trial 69 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 169, 'max_depth': 16}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:42,087] Trial 70 finished with value: 0.7783985102420856 and parameters: {'n_estimators': 195, 'max_depth': 17}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:43,132] Trial 71 finished with value: 0.7616387337057727 and parameters: {'n_estimators': 198, 'max_depth': 9}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:43,548] Trial 72 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 98, 'max_depth': 4}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:44,317] Trial 73 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 174, 'max_depth': 11}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:45,225] Trial 74 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 181, 'max_depth': 19}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:45,541] Trial 75 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 62, 'max_depth': 11}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:46,364] Trial 76 finished with value: 0.7616387337057727 and parameters: {'n_estimators': 197, 'max_depth': 4}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:47,147] Trial 77 finished with value: 0.7765363128491619 and parameters: {'n_estimators': 171, 'max_depth': 17}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:47,902] Trial 78 finished with value: 0.7597765363128491 and parameters: {'n_estimators': 198, 'max_depth': 3}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:48,306] Trial 79 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 90, 'max_depth': 18}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:48,827] Trial 80 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 117, 'max_depth': 12}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:49,144] Trial 81 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 72, 'max_depth': 7}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:49,353] Trial 82 finished with value: 0.7579143389199254 and parameters: {'n_estimators': 53, 'max_depth': 3}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:50,012] Trial 83 finished with value: 0.7746741154562383 and parameters: {'n_estimators': 150, 'max_depth': 7}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:50,923] Trial 84 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 187, 'max_depth': 20}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:51,745] Trial 85 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 152, 'max_depth': 17}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:52,336] Trial 86 finished with value: 0.7579143389199254 and parameters: {'n_estimators': 143, 'max_depth': 3}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:52,883] Trial 87 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 102, 'max_depth': 18}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:53,152] Trial 88 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 56, 'max_depth': 13}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:53,805] Trial 89 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 143, 'max_depth': 15}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:54,156] Trial 90 finished with value: 0.756052141527002 and parameters: {'n_estimators': 92, 'max_depth': 3}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:54,999] Trial 91 finished with value: 0.7560521415270017 and parameters: {'n_estimators': 199, 'max_depth': 6}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:55,744] Trial 92 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 163, 'max_depth': 18}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:56,237] Trial 93 finished with value: 0.7560521415270017 and parameters: {'n_estimators': 127, 'max_depth': 3}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:56,549] Trial 94 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 69, 'max_depth': 8}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:57,303] Trial 95 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 164, 'max_depth': 13}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:57,741] Trial 96 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 103, 'max_depth': 17}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:58,071] Trial 97 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 116, 'max_depth': 8}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:58,514] Trial 98 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 148, 'max_depth': 11}. Best is trial 11 with value: 0.7858472998137803.\n",
      "[I 2025-07-02 18:02:59,020] Trial 99 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 171, 'max_depth': 16}. Best is trial 11 with value: 0.7858472998137803.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial accuracy: 0.7858472998137803\n",
      "Best trial Hyperparameters: {'n_estimators': 121, 'max_depth': 15}\n",
      "Test accuracy ith best hyperparameters: 0.76\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T12:34:54.064199Z",
     "start_time": "2025-07-02T12:34:47.416339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we will use Grid Search\n",
    "\n",
    "# We have to explicitly mention our search space first\n",
    "\n",
    "search_space = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [5, 10, 15, 20]\n",
    "}\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.GridSampler(search_space))\n",
    "# As we are trying to 'Maximize' our accuracy so our direction of the objective function will be to maximize it\n",
    "study.optimize(objective, n_trials=100) # Running 50 trials to find the optimal hyperparameters\n",
    "\n",
    "# Print the best result\n",
    "print(f'Best trial accuracy: {study.best_trial.value}')\n",
    "print(f'Best trial Hyperparameters: {study.best_trial.params}')\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a RandomForestClassifier using the best hyperparameters from Optuna\n",
    "best_model = RandomForestClassifier(**study.best_trial.params, random_state=42)\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Test accuracy ith best hyperparameters: {test_accuracy:.2f}')"
   ],
   "id": "c1adc73c10ec3a62",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-02 18:04:47,419] A new study created in memory with name: no-name-f2b3e42d-d08b-4678-97e2-377437c3e93d\n",
      "[I 2025-07-02 18:04:47,731] Trial 0 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 0 with value: 0.7690875232774674.\n",
      "[I 2025-07-02 18:04:48,290] Trial 1 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 150, 'max_depth': 10}. Best is trial 0 with value: 0.7690875232774674.\n",
      "[I 2025-07-02 18:04:48,497] Trial 2 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 50, 'max_depth': 15}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-07-02 18:04:48,832] Trial 3 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-07-02 18:04:49,232] Trial 4 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-07-02 18:04:49,430] Trial 5 finished with value: 0.7579143389199254 and parameters: {'n_estimators': 50, 'max_depth': 10}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-07-02 18:04:49,941] Trial 6 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 150, 'max_depth': 5}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-07-02 18:04:50,443] Trial 7 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 150, 'max_depth': 20}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-07-02 18:04:50,889] Trial 8 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 150, 'max_depth': 15}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-07-02 18:04:51,478] Trial 9 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 200, 'max_depth': 10}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-07-02 18:04:52,089] Trial 10 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 200, 'max_depth': 20}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-07-02 18:04:52,730] Trial 11 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 200, 'max_depth': 15}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-07-02 18:04:53,342] Trial 12 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 200, 'max_depth': 5}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-07-02 18:04:53,513] Trial 13 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 50, 'max_depth': 5}. Best is trial 13 with value: 0.7746741154562384.\n",
      "[I 2025-07-02 18:04:53,852] Trial 14 finished with value: 0.7616387337057727 and parameters: {'n_estimators': 100, 'max_depth': 10}. Best is trial 13 with value: 0.7746741154562384.\n",
      "[I 2025-07-02 18:04:54,013] Trial 15 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 50, 'max_depth': 20}. Best is trial 13 with value: 0.7746741154562384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial accuracy: 0.7746741154562384\n",
      "Best trial Hyperparameters: {'n_estimators': 50, 'max_depth': 5}\n",
      "Test accuracy ith best hyperparameters: 0.74\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Optuna Visualization",
   "id": "6517a1dcc3c29dc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T12:40:05.594127Z",
     "start_time": "2025-07-02T12:40:04.773399Z"
    }
   },
   "cell_type": "code",
   "source": "from optuna.visualization import plot_optimization_history, plot_parallel_coordinate, plot_slice, plot_contour, plot_param_importances",
   "id": "2893885ecd4793a5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T12:37:58.780155Z",
     "start_time": "2025-07-02T12:37:56.252800Z"
    }
   },
   "cell_type": "code",
   "source": "# !uv pip install matplotlib plotly",
   "id": "7a94ac76c265200e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2mUsing Python 3.10.12 environment at: PyTorch\u001B[0m\r\n",
      "\u001B[2K\u001B[2mResolved \u001B[1m13 packages\u001B[0m \u001B[2min 183ms\u001B[0m\u001B[0m                                        \u001B[0m\r\n",
      "\u001B[2K\u001B[2mPrepared \u001B[1m2 packages\u001B[0m \u001B[2min 2.16s\u001B[0m\u001B[0m                                             \r\n",
      "\u001B[2K\u001B[2mInstalled \u001B[1m2 packages\u001B[0m \u001B[2min 25ms\u001B[0m\u001B[0m                                \u001B[0m\r\n",
      " \u001B[32m+\u001B[39m \u001B[1mnarwhals\u001B[0m\u001B[2m==1.45.0\u001B[0m\r\n",
      " \u001B[32m+\u001B[39m \u001B[1mplotly\u001B[0m\u001B[2m==6.2.0\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T12:40:05.806110Z",
     "start_time": "2025-07-02T12:40:05.613202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Optimization history - trial number vs accuracy graph\n",
    "\n",
    "plot_optimization_history(study).show()"
   ],
   "id": "48cd207769b4e115",
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "markers",
         "name": "Objective Value",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.7783985102420856,
          0.7690875232774674,
          0.756052141527002,
          0.7653631284916201,
          0.7709497206703911,
          0.7653631284916201,
          0.7597765363128491,
          0.7690875232774674,
          0.7765363128491621,
          0.7597765363128491,
          0.7783985102420857,
          0.7765363128491621,
          0.7653631284916201,
          0.7746741154562384,
          0.7783985102420856,
          0.7728119180633147,
          0.7709497206703911,
          0.7690875232774674,
          0.7672253258845437,
          0.7783985102420857,
          0.7783985102420857,
          0.7746741154562384,
          0.7709497206703911,
          0.7746741154562384,
          0.7709497206703911,
          0.7709497206703911,
          0.7746741154562384,
          0.7653631284916201,
          0.7746741154562384,
          0.7765363128491621,
          0.7653631284916201,
          0.7690875232774674,
          0.7672253258845437,
          0.7839851024208566,
          0.7783985102420857,
          0.7653631284916201,
          0.7746741154562384,
          0.7765363128491619,
          0.7709497206703911,
          0.7690875232774674,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7690875232774674,
          0.7728119180633147,
          0.7783985102420857,
          0.7690875232774674,
          0.7597765363128492,
          0.7765363128491621,
          0.7802607076350093,
          0.7746741154562384,
          0.7765363128491621,
          0.7765363128491619,
          0.7690875232774674,
          0.7728119180633147,
          0.7690875232774674,
          0.7690875232774674,
          0.7690875232774674,
          0.7653631284916201,
          0.7728119180633147,
          0.7821229050279331,
          0.7783985102420857,
          0.7635009310986964,
          0.7709497206703911,
          0.7821229050279331,
          0.7709497206703911,
          0.7821229050279331,
          0.7821229050279331,
          0.7746741154562384,
          0.7709497206703911,
          0.7709497206703911,
          0.7765363128491621,
          0.7709497206703911,
          0.7672253258845437,
          0.7653631284916201,
          0.7635009310986964,
          0.7746741154562384,
          0.7653631284916201,
          0.7728119180633147,
          0.7728119180633147,
          0.7802607076350094,
          0.7802607076350094,
          0.7765363128491621,
          0.7746741154562384,
          0.7746741154562384,
          0.7653631284916201,
          0.7746741154562384,
          0.7690875232774674,
          0.7690875232774674,
          0.7541899441340782,
          0.7690875232774674,
          0.7783985102420857,
          0.7783985102420857,
          0.7709497206703911,
          0.7783985102420857,
          0.7690875232774674,
          0.7728119180633147,
          0.7672253258845437,
          0.7746741154562384,
          0.7728119180633147
         ],
         "type": "scatter"
        },
        {
         "mode": "lines",
         "name": "Best Value",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.7783985102420856,
          0.7783985102420856,
          0.7783985102420856,
          0.7783985102420856,
          0.7783985102420856,
          0.7783985102420856,
          0.7783985102420856,
          0.7783985102420856,
          0.7783985102420856,
          0.7783985102420856,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7783985102420857,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566,
          0.7839851024208566
         ],
         "type": "scatter"
        },
        {
         "marker": {
          "color": "#cccccc"
         },
         "mode": "markers",
         "name": "Infeasible Trial",
         "showlegend": false,
         "x": [],
         "y": [],
         "type": "scatter"
        }
       ],
       "layout": {
        "title": {
         "text": "Optimization History Plot"
        },
        "xaxis": {
         "title": {
          "text": "Trial"
         }
        },
        "yaxis": {
         "title": {
          "text": "Objective Value"
         }
        },
        "template": {
         "data": {
          "histogram2dcontour": [
           {
            "type": "histogram2dcontour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "choropleth": [
           {
            "type": "choropleth",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "histogram2d": [
           {
            "type": "histogram2d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmap": [
           {
            "type": "heatmap",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "contourcarpet": [
           {
            "type": "contourcarpet",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "contour": [
           {
            "type": "contour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "surface": [
           {
            "type": "surface",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "mesh3d": [
           {
            "type": "mesh3d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "parcoords": [
           {
            "type": "parcoords",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolargl": [
           {
            "type": "scatterpolargl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "scattergeo": [
           {
            "type": "scattergeo",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolar": [
           {
            "type": "scatterpolar",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "scattergl": [
           {
            "type": "scattergl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatter3d": [
           {
            "type": "scatter3d",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermap": [
           {
            "type": "scattermap",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermapbox": [
           {
            "type": "scattermapbox",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterternary": [
           {
            "type": "scatterternary",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattercarpet": [
           {
            "type": "scattercarpet",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ]
         },
         "layout": {
          "autotypenumbers": "strict",
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "hovermode": "closest",
          "hoverlabel": {
           "align": "left"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "bgcolor": "#E5ECF6",
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "ternary": {
           "bgcolor": "#E5ECF6",
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "sequential": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ]
          },
          "xaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "automargin": true,
           "zerolinewidth": 2
          },
          "yaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "automargin": true,
           "zerolinewidth": 2
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "geo": {
           "bgcolor": "white",
           "landcolor": "#E5ECF6",
           "subunitcolor": "white",
           "showland": true,
           "showlakes": true,
           "lakecolor": "white"
          },
          "title": {
           "x": 0.05
          },
          "mapbox": {
           "style": "light"
          }
         }
        }
       },
       "config": {
        "plotlyServerURL": "https://plot.ly"
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T12:47:10.759352Z",
     "start_time": "2025-07-02T12:47:10.728160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Parallel Coordinate Plot\n",
    "\n",
    "plot_parallel_coordinate(study).show()"
   ],
   "id": "8d20efa4867d4802",
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "dimensions": [
          {
           "label": "Objective Value",
           "range": [
            0.7541899441340782,
            0.7839851024208566
           ],
           "values": [
            0.7783985102420856,
            0.7690875232774674,
            0.756052141527002,
            0.7653631284916201,
            0.7709497206703911,
            0.7653631284916201,
            0.7597765363128491,
            0.7690875232774674,
            0.7765363128491621,
            0.7597765363128491,
            0.7783985102420857,
            0.7765363128491621,
            0.7653631284916201,
            0.7746741154562384,
            0.7783985102420856,
            0.7728119180633147,
            0.7709497206703911,
            0.7690875232774674,
            0.7672253258845437,
            0.7783985102420857,
            0.7783985102420857,
            0.7746741154562384,
            0.7709497206703911,
            0.7746741154562384,
            0.7709497206703911,
            0.7709497206703911,
            0.7746741154562384,
            0.7653631284916201,
            0.7746741154562384,
            0.7765363128491621,
            0.7653631284916201,
            0.7690875232774674,
            0.7672253258845437,
            0.7839851024208566,
            0.7783985102420857,
            0.7653631284916201,
            0.7746741154562384,
            0.7765363128491619,
            0.7709497206703911,
            0.7690875232774674,
            0.7783985102420857,
            0.7783985102420857,
            0.7783985102420857,
            0.7690875232774674,
            0.7728119180633147,
            0.7783985102420857,
            0.7690875232774674,
            0.7597765363128492,
            0.7765363128491621,
            0.7802607076350093,
            0.7746741154562384,
            0.7765363128491621,
            0.7765363128491619,
            0.7690875232774674,
            0.7728119180633147,
            0.7690875232774674,
            0.7690875232774674,
            0.7690875232774674,
            0.7653631284916201,
            0.7728119180633147,
            0.7821229050279331,
            0.7783985102420857,
            0.7635009310986964,
            0.7709497206703911,
            0.7821229050279331,
            0.7709497206703911,
            0.7821229050279331,
            0.7821229050279331,
            0.7746741154562384,
            0.7709497206703911,
            0.7709497206703911,
            0.7765363128491621,
            0.7709497206703911,
            0.7672253258845437,
            0.7653631284916201,
            0.7635009310986964,
            0.7746741154562384,
            0.7653631284916201,
            0.7728119180633147,
            0.7728119180633147,
            0.7802607076350094,
            0.7802607076350094,
            0.7765363128491621,
            0.7746741154562384,
            0.7746741154562384,
            0.7653631284916201,
            0.7746741154562384,
            0.7690875232774674,
            0.7690875232774674,
            0.7541899441340782,
            0.7690875232774674,
            0.7783985102420857,
            0.7783985102420857,
            0.7709497206703911,
            0.7783985102420857,
            0.7690875232774674,
            0.7728119180633147,
            0.7672253258845437,
            0.7746741154562384,
            0.7728119180633147
           ]
          },
          {
           "label": "max_depth",
           "range": [
            3,
            20
           ],
           "values": [
            18,
            13,
            4,
            11,
            10,
            11,
            6,
            15,
            12,
            3,
            19,
            20,
            20,
            17,
            17,
            18,
            15,
            8,
            15,
            18,
            20,
            20,
            18,
            16,
            19,
            14,
            19,
            17,
            19,
            18,
            16,
            20,
            13,
            18,
            19,
            16,
            17,
            18,
            14,
            8,
            19,
            19,
            20,
            18,
            19,
            17,
            20,
            10,
            19,
            16,
            16,
            18,
            17,
            20,
            5,
            18,
            15,
            20,
            17,
            12,
            19,
            19,
            19,
            18,
            16,
            14,
            16,
            16,
            15,
            16,
            13,
            16,
            15,
            14,
            17,
            16,
            17,
            15,
            16,
            18,
            17,
            17,
            17,
            18,
            16,
            15,
            17,
            17,
            14,
            3,
            13,
            18,
            18,
            16,
            19,
            8,
            18,
            7,
            17,
            19
           ]
          },
          {
           "label": "n_estimators",
           "range": [
            50,
            200
           ],
           "values": [
            106,
            59,
            173,
            130,
            200,
            102,
            148,
            156,
            169,
            111,
            71,
            70,
            82,
            91,
            119,
            52,
            81,
            131,
            99,
            67,
            72,
            68,
            50,
            71,
            81,
            63,
            91,
            78,
            89,
            111,
            60,
            58,
            97,
            73,
            72,
            60,
            77,
            198,
            90,
            138,
            65,
            71,
            75,
            56,
            105,
            67,
            85,
            95,
            74,
            115,
            156,
            120,
            171,
            85,
            53,
            182,
            64,
            108,
            83,
            139,
            73,
            71,
            78,
            50,
            63,
            56,
            63,
            63,
            63,
            59,
            67,
            61,
            67,
            55,
            78,
            88,
            116,
            94,
            75,
            81,
            70,
            70,
            69,
            63,
            73,
            53,
            57,
            87,
            101,
            80,
            61,
            69,
            75,
            66,
            71,
            128,
            64,
            84,
            77,
            160
           ]
          }
         ],
         "labelangle": 30,
         "labelside": "bottom",
         "line": {
          "color": [
           0.7783985102420856,
           0.7690875232774674,
           0.756052141527002,
           0.7653631284916201,
           0.7709497206703911,
           0.7653631284916201,
           0.7597765363128491,
           0.7690875232774674,
           0.7765363128491621,
           0.7597765363128491,
           0.7783985102420857,
           0.7765363128491621,
           0.7653631284916201,
           0.7746741154562384,
           0.7783985102420856,
           0.7728119180633147,
           0.7709497206703911,
           0.7690875232774674,
           0.7672253258845437,
           0.7783985102420857,
           0.7783985102420857,
           0.7746741154562384,
           0.7709497206703911,
           0.7746741154562384,
           0.7709497206703911,
           0.7709497206703911,
           0.7746741154562384,
           0.7653631284916201,
           0.7746741154562384,
           0.7765363128491621,
           0.7653631284916201,
           0.7690875232774674,
           0.7672253258845437,
           0.7839851024208566,
           0.7783985102420857,
           0.7653631284916201,
           0.7746741154562384,
           0.7765363128491619,
           0.7709497206703911,
           0.7690875232774674,
           0.7783985102420857,
           0.7783985102420857,
           0.7783985102420857,
           0.7690875232774674,
           0.7728119180633147,
           0.7783985102420857,
           0.7690875232774674,
           0.7597765363128492,
           0.7765363128491621,
           0.7802607076350093,
           0.7746741154562384,
           0.7765363128491621,
           0.7765363128491619,
           0.7690875232774674,
           0.7728119180633147,
           0.7690875232774674,
           0.7690875232774674,
           0.7690875232774674,
           0.7653631284916201,
           0.7728119180633147,
           0.7821229050279331,
           0.7783985102420857,
           0.7635009310986964,
           0.7709497206703911,
           0.7821229050279331,
           0.7709497206703911,
           0.7821229050279331,
           0.7821229050279331,
           0.7746741154562384,
           0.7709497206703911,
           0.7709497206703911,
           0.7765363128491621,
           0.7709497206703911,
           0.7672253258845437,
           0.7653631284916201,
           0.7635009310986964,
           0.7746741154562384,
           0.7653631284916201,
           0.7728119180633147,
           0.7728119180633147,
           0.7802607076350094,
           0.7802607076350094,
           0.7765363128491621,
           0.7746741154562384,
           0.7746741154562384,
           0.7653631284916201,
           0.7746741154562384,
           0.7690875232774674,
           0.7690875232774674,
           0.7541899441340782,
           0.7690875232774674,
           0.7783985102420857,
           0.7783985102420857,
           0.7709497206703911,
           0.7783985102420857,
           0.7690875232774674,
           0.7728119180633147,
           0.7672253258845437,
           0.7746741154562384,
           0.7728119180633147
          ],
          "colorbar": {
           "title": {
            "text": "Objective Value"
           }
          },
          "colorscale": [
           [
            0.0,
            "rgb(247,251,255)"
           ],
           [
            0.125,
            "rgb(222,235,247)"
           ],
           [
            0.25,
            "rgb(198,219,239)"
           ],
           [
            0.375,
            "rgb(158,202,225)"
           ],
           [
            0.5,
            "rgb(107,174,214)"
           ],
           [
            0.625,
            "rgb(66,146,198)"
           ],
           [
            0.75,
            "rgb(33,113,181)"
           ],
           [
            0.875,
            "rgb(8,81,156)"
           ],
           [
            1.0,
            "rgb(8,48,107)"
           ]
          ],
          "reversescale": false,
          "showscale": true
         },
         "type": "parcoords"
        }
       ],
       "layout": {
        "title": {
         "text": "Parallel Coordinate Plot"
        },
        "template": {
         "data": {
          "histogram2dcontour": [
           {
            "type": "histogram2dcontour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "choropleth": [
           {
            "type": "choropleth",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "histogram2d": [
           {
            "type": "histogram2d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmap": [
           {
            "type": "heatmap",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "contourcarpet": [
           {
            "type": "contourcarpet",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "contour": [
           {
            "type": "contour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "surface": [
           {
            "type": "surface",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "mesh3d": [
           {
            "type": "mesh3d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "parcoords": [
           {
            "type": "parcoords",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolargl": [
           {
            "type": "scatterpolargl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "scattergeo": [
           {
            "type": "scattergeo",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolar": [
           {
            "type": "scatterpolar",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "scattergl": [
           {
            "type": "scattergl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatter3d": [
           {
            "type": "scatter3d",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermap": [
           {
            "type": "scattermap",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermapbox": [
           {
            "type": "scattermapbox",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterternary": [
           {
            "type": "scatterternary",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattercarpet": [
           {
            "type": "scattercarpet",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ]
         },
         "layout": {
          "autotypenumbers": "strict",
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "hovermode": "closest",
          "hoverlabel": {
           "align": "left"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "bgcolor": "#E5ECF6",
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "ternary": {
           "bgcolor": "#E5ECF6",
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "sequential": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ]
          },
          "xaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "automargin": true,
           "zerolinewidth": 2
          },
          "yaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "automargin": true,
           "zerolinewidth": 2
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "geo": {
           "bgcolor": "white",
           "landcolor": "#E5ECF6",
           "subunitcolor": "white",
           "showland": true,
           "showlakes": true,
           "lakecolor": "white"
          },
          "title": {
           "x": 0.05
          },
          "mapbox": {
           "style": "light"
          }
         }
        }
       },
       "config": {
        "plotlyServerURL": "https://plot.ly"
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Define by run is used in Optuna to search from Dynamic Search Spaces. We can consider the models/ML algorithms as hyperparameter which gives the flexibility to know which ML model is the best for our problem and also gives the best hyperparameters for that particular algorithm. - This challenge is solved by Optuna!!! Every algorithm will have different/dynamic search space wrt the algorith it is using. Optuna also supports distributed computing to make the computing faster.",
   "id": "ef42abcebf5e97df"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
